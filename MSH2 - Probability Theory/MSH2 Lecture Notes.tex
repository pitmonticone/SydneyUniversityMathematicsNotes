% Created by Andrew Tulloch

%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode


\documentclass[10pt, oneside, reqno]{amsart}
\usepackage{geometry, setspace, graphicx, enumerate}
\onehalfspacing                 
\usepackage{fontspec,xltxtra,xunicode}
\defaultfontfeatures{Mapping=tex-text}

% AMS Theorems
\theoremstyle{plain}% default 
\newtheorem{thm}{Theorem}[section] 
\newtheorem{lem}[thm]{Lemma} 
\newtheorem{prop}[thm]{Proposition} 
\newtheorem{cor}[thm]{Corollary} 


\newcommand{\res}[2]{\text{Res}(#1,#2)}
\theoremstyle{definition} 
\newtheorem{defn}[thm]{Definition}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{exmp}[thm]{Example}
% \newtheorem{exer}[thm][Exercise]

\theoremstyle{remark} 
\newtheorem*{rem}{Remark} 
\newtheorem*{note}{Note} 
\newtheorem{case}{Case} 

\newcommand{\expc}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\var}{\text{Var}}
\newcommand{\cov}[1]{\text{Cov}\left(#1\right)}
\newcommand{\prob}[1]{\mathbb{P}(#1)}
\newcommand{\given}{ \, | \,}
\newcommand{\us}{0 \leq u \leq s}
\newcommand{\ts}[1]{\{ #1 \}}

\renewcommand{\phi}{\varphi}
\newcommand{\sigf}{\mathcal{F}}

\newcommand{\dzz}{\, dz}
\newcommand{\bigo}[1]{\mathcal{O}(#1)}

\newcommand{\al}{\alpha}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathbb{N}}

\newcommand{\I}{\mathbb{I}}

\renewcommand{\P}{\mathbb{P}}

\newcommand{\F}{\mathbb{F}}
\newcommand{\Ga}{\mathbb{G}}

\newcommand{\aut}[1]{\text{Aut}{(#1)}}

\newcommand{\gener}[1]{\langle #1 \rangle}
\newcommand{\charr}[1]{\text{char}(#1)}
\newcommand{\nth}{n\textsuperscript{th}}

% \newcommand{\limsup}{\text{limsup}}

\newcommand{\tworow}[2]{\genfrac{}{}{0pt}{}{#1}{#2}}
\newcommand{\xdeg}[2]{[#1 : #2]}
\newcommand{\gal}[2]{\text{Gal}(#1/#2)}
\newcommand{\minpoly}[2]{m_{#1, #2}(x)}

\newcommand{\mapping}[5]{\begin{align*}
    #1 : \quad     #2 &\rightarrow #3 \\
            #4  &\mapsto #5
\end{align*}    
}


\def\cip{\,{\buildrel p \over \rightarrow}\,} 
\def\cid{\,{\buildrel \!d \over \rightarrow}\,} 
\def\cas{\,{\buildrel a.s. \over \rightarrow}\,} 

\def\clp{\,{\buildrel L^p \over \rightarrow}\,} 
\def\clone{\,{\buildrel L^1 \over \rightarrow}\,} 

\def\eqd{\,{\buildrel \!d \over =}\,} 
\def\eqas{\,{\buildrel a.s. \over =}\,}
\newcommand{\sigg}{\mathcal{G}}     
\newcommand{\indic}[1]{\mathbf{1}_{\{ #1 \}} }
\newcommand{\sumni}{\sum_{i=1}^n}
\newcommand{\sumnj}{\sum_{j=1}^n}
\newcommand{\sumnk}{\sum_{k=1}^n}
        
\usepackage{hyperref}
        
\title{MSH2 - Probability Theory}                               % Document Title
\author{Andrew Tulloch}
%\date{}                                           % Activate to display a given date or no date


\begin{document}
\maketitle \tableofcontents \clearpage


\section{Lecture 1 - Thursday 3 March} % (fold)
\label{sec:lecture_1_thursday_3_march}

\begin{defn}[$\sigma$-field]
    Let $\Omega$ be a non-empty set.  Let $\mathcal{F}$ be a collection of subsets of $\Omega$.  We call $\mathcal{F}$ a $\sigma$-field if 
    \begin{itemize}
        \item $\emptyset \in \mathcal{F}$,
        \item $A \in \mathcal{F} \Rightarrow A^c \in \mathcal{F}$,
        \item $A, B \in \sigf \Rightarrow A \cup B \in \sigf$
        \item If $(A_i) \in \sigf$, then $\bigcup_{j = 1}^\infty A_j \in \sigf$.
    \end{itemize}
\end{defn}

\begin{defn}[Probability measure]
    Let $\P$ be a function on $\sigf$ satisfying
    \begin{itemize}
        \item If $A \in \sigf$ then $\P(A) \geq 0$,
        \item $P(\Omega) = 1$,
        \item If $(A_j) \in \sigf$ and $A_i \cap A_j = \emptyset$, then $\P \left( \bigcup_{j=1}^\infty A_j \right) = \sum_{j=1}^\infty \P(A_j)$.
    \end{itemize}
    Then we call $\P$ a \textbf{probability measure} on $\sigf$.
\end{defn}

\begin{defn}[$\sigma$-field generated by a set]
    If $A$ is a class of sets, then $\sigma(A)$ is the smallest $\sigma$-field that contains $A$.
\end{defn}

\begin{exmp}
    For a set $B$, $\sigma(B) = \{ \emptyset, \Omega, B, B^c \}$.
\end{exmp}

\begin{defn}[Borel $\sigma$-field] Let $\mathcal{B}$ be the class of all \textbf{finite} unions of intervals of the form $(a, b]$ on $\R$.  The $\sigma$-field $\sigma(\mathcal{B})$ is called the \textbf{Borel $\sigma$-field}.
    
    Note that $\mathcal{B}$ itself is not a $\sigma$-field - consider $\bigcup_{j=1}^\infty (0, \frac{1}{2} - \frac{1}{j}] = (0, \frac{1}{2}) \notin \mathcal{B}$.
\end{defn}

\subsection{Constructing extensions of functions to form probability measures} % (fold)
\label{sub:constructing_extensions_of_functions_to_form_probability_measures}

% subsection constructing_extensions_of_functions_to_form_probability_measures (end)

\begin{lem}[Continuity property]
    Let $\mathcal{A}$ be a field of subsets of $\Omega$.  Assume $\emptyset \in \mathcal{A}$ and that $\mathcal{A}$ is closed under complements and finite unions.

    If $A_j \in \sigf$ and $A_{j+1} \subset A_j$ with $\bigcap_{j=1}^\infty A_j = \emptyset$, then $\lim_{j \rightarrow \infty} \P(A_j) = 0$.
\end{lem}

\begin{thm}
    Let $\sigma( \mathcal{A})$ be the $\sigma$-field generated by $\mathcal{A}$.  If the continuity property holds, then there is a \textbf{unique} probability measure on $\sigma(\mathcal{A})$ which is an extension of $\P$, i.e. the measures agree on all elements of $\mathcal{A}$.  
\end{thm}


\begin{defn}[Limits of sets] Let $(\Omega, \sigf, \P)$ be a probability space, and assume $(A_i) \in \sigf$.  Then define $\limsup_{m \rightarrow \infty} A_n$ as \[
    \limsup_{n \rightarrow \infty} A_n = \bigcap_{n = 1}^\infty \bigcup_{m \geq n} A_m = \overline{\lim} A_n
\] 
An element $\omega \in \overline{\lim} A_n$ if and only if $\omega \in A_m$ for some $m \geq n$ for all $n$ - that is, $\omega$ is in infinitely many of the sets $A_m$.

Similarly, define $\liminf_{m \rightarrow \infty} A_n$ as \[
    \liminf_{n \rightarrow \infty} A_n = \bigcup_{n = 1}^\infty \bigcap_{m \geq n} A_m = \underline{\lim} A_n
\] 
An element $\omega \in \underline{\lim} A_n$ if and only if $\omega$ is in all but a finite number of sets $A_m$.  

Clearly, \[
    \underline{\lim}A_n \subseteq \overline{\lim} A_n
\]
If $\underline{\lim}A_n$ and $\overline{\lim}A_n$ coincide we write it as $\lim A_n$.
\end{defn}

\begin{lem}\label{lem:problim}
    Assume the continuity property holds.  If $A_n \downarrow A$ then $\P(A_n) \downarrow \P(A)$, and if $A_n \uparrow A$ then $\P(A_n) \uparrow \P(A)$. 
    
\end{lem}

\begin{proof}
    If $A_n \downarrow A$, then $A_n \supseteq A_{n+1} \dots$ and $\bigcap_{n=1}^\infty A_n = A$.  We can write $A_n = (A_n - A) \cup A$.  Then we have 
    \begin{align*}
        \P(A_n) = \P(A_n - A) + \P(A) \\
        \P(A_n) \geq \P(A)
    \end{align*}
    By the continuity property, $\P(A_n - A) \rightarrow 0$, and so $\P(A_n) \downarrow \P(A)$.
\end{proof}

% section lecture_1_thursday_3_march (end)

\section{Lecture 2 - Thursday 3 March} % (fold)
\label{sec:lecture_2_thursday_3_march}
\begin{thm}
    \[
        \P( \underline{\lim} A_n ) \leq \underline{\lim} \P(A_n) \leq \overline{\lim} \P(A_n) \leq \P( \overline{\lim} A_n )    
    \]
\end{thm}
\begin{proof}
    We know $A_n \downarrow \underline{\lim} A_n$, and so from Lemma \ref{lem:problim} we have that $a$.
\end{proof}

\begin{defn}[Measurable function]
    Let $(\Omega, \sigf, \P)$ be a probability space.  Let $X : \Omega \rightarrow \R$ be real valued function on $\Omega$.  Then $X$ is \textbf{measurable} with respect to $\sigf$ if $X^{-1}(B)$ is an element of $\sigf$ for every $B$ in the Borel $\sigma$-field of $\R$. 
\end{defn}

\begin{defn}[Random variable]
    A random variable is a measurable function from $\Omega$ to $\R$.  
\end{defn}

\begin{defn}[Expectation]
    If $\int_\Omega | X(\omega) | \, d\P < \infty$ then we can define $\E(X) = \int_\Omega X(\omega) \, d \P$
\end{defn}

\begin{defn}[Distribution]
    $X$ induces a probability measure $\P_X$ on $\R$\[
        \P_X(B) = \P(X^{-1}(S)), S \in \mathcal{B}
    \]
    $P_X$ is called the \textbf{distribution} of $X$.  $(\R, \mathcal{B}, \P_X)$ is a probability space.
    The distribution function $F_X(x) = \P( \{ \omega : X(\omega) \leq x \} ) = \P_X( (-\infty, x] )$.  
    We have $\E(X) = \int_\R x \, dP_X(x) = \int_\R x \, dF_X(x)$.
\end{defn}

\subsection{Key results from Measure Theory} % (fold)
\label{sub:key_results_from_measure_theory}

\begin{thm}[Monotone convergence theorem]
    If $0 \leq X_n \uparrow X a.s$ then $0 \leq \E(X_n) \uparrow \E(X)$ where $\E(X)$ is infinite if $\E(X_n) \uparrow \infty$.
\end{thm}

\begin{thm}[Dominated convergence theorem]
    If $\lim X_n = X a.s.$ and $|X_n | \leq Y$ for all $n \geq 1$, with $\E(|Y|) < \infty$ then $\lim \E(X_n) = \E(X)$.
\end{thm}

\begin{thm}[Fatau's Lemma]
    If $X_n \geq Y$ for all $n$ with $\E(|Y|) < \infty$ then \[
        \E( \liminf X_n) \leq \liminf \E(X_n)
    \]
\end{thm}

% subsection key_results_from_measure_theory (end)

\begin{thm}[Composition]
    Let $(\Omega, \sigf, \P)$ and $(\Omega', \sigf')$ be spaces.  Let $\Phi: \Sigma \rightarrow \Sigma'$ be measurable.  Define $\P_\Phi$ on $\sigf$ by $\P_\Phi(M) = \P(\Phi^{-1}(M))$.  Let $X'$ be a measurable function from $\Sigma'$ to $\R$.  Then $X(\omega) = X'(\Phi(\omega))$ is a measurable function.  Then we have 
    \[
        \E(X) = \int_{\Omega'} X' \, d\P_\phi
    \]
\end{thm}

\begin{proof}
    Suppose $X'$ is an indictor function for $A \in \sigf'$.  Then \[
        \int_{\Omega'} X' \, d\P_\phi = \int_A \, d\P_\phi = \P_\phi(A) = \P(\phi^{-1}(A)) = \E(X)
    \]
    So the result is true for simple functions.  
    
    Now, suppose $X' \geq 0$.  Then there exists a pointwise increasing sequence of simple functions $X'_n$ such that $X_n' \rightarrow X'$.  By the monotone convergence theorem, we know \[
        \lim_{n \rightarrow \infty} \int_{\Omega'} X'_n \, d\P_\phi = \int_{\Omega'} X' \, d\P_\phi
    \] 
    But $X_n(\omega) = X_n'(\Phi(\omega) )$ are also simple functions increasing to $X$.  Hence, we know that $\lim_{n \rightarrow \infty} \E(X_n) = \E(X)$.
\end{proof}

% section lecture_2_thursday_3_march (end)

\section{Lecture 3 - Thursday 10 March} % (fold)
\label{sec:lecture_3_thursday_10_march}
\begin{thm}[Jensen's inequality]
    Let $\phi(x)$ be a convex function on $\R$.  Let $X$ be a random variable.  Assume $\E(X) < \infty$, $\E(\phi(X)) < \infty$.  Then \[
        \phi(E(X)) \leq \E(\phi(X))
    \]
\end{thm}

\begin{thm}[H\"older's inequality]
    Let $1 < p < \infty$ and $\frac{1}{p} + \frac{1}{q} = 1$. Then we have\[
        | \E(XY) | \leq \E(|XY|) \leq \left( \E(|X|^p) \right)^{1/p} \left( \E(|Y|^q) \right)^{1/q}
    \] 
    If $p = q = 2$ we obtain the Cauchy-Swartz inequality $\E(|XY|) \leq \left( \E(|X|^2) \right)^{1/2} \left( \E(|Y|^2) \right)^{1/2}$.
    
    If $Y = 1$ then $\E(|X|) \leq ( \E(|X|^p))^{1/p}$.
\end{thm}
\begin{proof}
    Let $W$ be a random variable taking values $a_1$ with probability $1/p$, $a_2$ with probability $1/q$, with $1/p + 1/q = 1$.  Applying Jensen's inequality with $\phi(x) = -\log(x)$ gives 
    \begin{align*}
        \E( - \log W) \geq -\log \E(W) \\
        \frac{1}{p} (\log a_1) + \frac{1}{q} ( -\log a_2) \geq -\log( \frac{a_1}{p} + \frac{a_2}{q}) \\
        -\log(a_1^{1/p} \cdot a_2^{1/q}) \geq -\log( \frac{a_1}{p} + \frac{a_2}{q})  \\
        a_1^{1/p} \cdot a_2^{1/q} \leq \frac{a_1}{p} + \frac{a_2}{q}
    \end{align*}
    Where the inequality is trivial if $a_1$ or $a_2$ is zero.
    
    Setting $a_1 = |x|^p$ and $a_2 = |y|^q$, we obtain \[
        |xy| \leq \frac{|x|^p}{p} + \frac{|y|^q}{q}.
    \]
    
    Let $x = \frac{X}{(\E(|X|^p))^1/p}$ and $y = \frac{Y}{(\E(|Y|^q))^{1/q}}$
or take expectations across the inequality, we obtain \[
    \E(|XY|) \leq \left( \E(|X|^p) \right)^{1/p} \left( \E(|Y|^q) \right)^{1/q}
\]
\end{proof}


\begin{exmp}
    If $1 < r < r'$ then $\frac{r'}{r} > 1$.  Then \[
        \E(|X|^r) \leq (\E((|X|^r)^{r'/r}))^{1/(r'/r)} = (\E(|X|^{r'}))^{r'/r}
    \]
\end{exmp}

\begin{thm}[Liapounov's inequality]
    \[
        (\E|X|^r)^{1/r} \leq ( \E(|X|^{r'}))^{1/r'}
    \]
\end{thm}
\begin{cor}
    Thus if $\E(|X|^r) < \infty$ then $X$ has all moments of lower order finite i.e. $\E(|X|^p) < \infty$ for all $ 1 \leq p \leq r$
\end{cor}


\begin{thm}[Minkowski's inequality]
    If $p \geq 1$, then \[
        (\E(|X+Y|^p))^{1/p} \leq (\E(|X|^p))^{1/p} + (\E(|Y|^p))^{1/p}
    \]
\end{thm}
\begin{proof}
    \begin{align*}
        \E(|X+Y|^p) \leq \E(|X| \cdot |X+Y|^{p-1} + \E(|Y| \cdot |X+Y|^{p-1}) \\
                    = \E(|X|^p)^{1/p} ( \E(|X+Y|^{p-1})^q)^{1/q} + \E(|Y|^p)^{1/p} ( \E(|X+Y|^{p-1})^q)^{1/q}
    \end{align*}
    Let $1/p + 1/q = 1$.  Then from H\"older, 
    \begin{align*}
        \E(|X+Y|^p) \leq (\E(|X+Y|^p))^1/q \cdot ( (\E(|X|^p))^{1/p} + ( \E(|Y|^p))^{1/p}
    \end{align*}
    and so \[
        (\E(|X+Y|^p))^{1/p} \leq (\E(|X|^p))^{1/p} + (\E(|Y|^p))^{1/p}
    \]
\end{proof}

\subsection{Modes of Convergence} % (fold)
\label{sub:modes_of_convergence}
Let $(\Omega, \sigf, \P)$ be a probability space and $X_n(\omega), n \geq 1$ is a sequence of random variables.  

\begin{defn}[Almost surely convergence]
    We say $X_n$ converges almost surely if \[
        \P( \{ \omega \, | \, X_n(\omega) \text{ has a limit} \} ) = 1
    \]
    We write $X_n \cas X$ where $X$ denotes the limiting random variable.
\end{defn}

\begin{defn}[Convergence in probability]
    $X_n$ converges in probability to $X$ \[
        X_n \cip X
    \] if for all $\epsilon > 0$, \[
        \P( \{ \omega \, | \, |X_n(\omega) - X(\omega) | > \epsilon \} ) \rightarrow 0
    \]
    or alternatively,\[
        \P( |X_n - X| > \epsilon) \rightarrow 0
    \]
\end{defn}

\begin{defn}[Convergence in mean]
    $X_n$ converges to $X$ in mean of order $p$ (or in $L^p$) if\[
        \E(|X_n - X|^p) \rightarrow 0
    \]
    We write $X_n \clp X$.  We note that for convergence of order $L^p$, we need $\E(|X_n|^p) < \infty$.
\end{defn}

\begin{thm}
    If $X_n \clp X$ then $X_n \cip X$ for any $p > 0$.
\end{thm}
% subsection modes_of_convergence (end)
% section lecture_3_thursday_10_march (end)

\section{Lecture 4 - Thursday 10 March} % (fold)
\label{sec:lecture_4_thursday_10_march}

\begin{lem}
    Let $C_1, C_2, \dots$ be sets in $\sigf$ and $\sum_n \P(C_n) < \infty$.  Then $\P(\overline{\lim} C_n) = 0$
\end{lem}
\begin{proof}
    Since $\overline{\lim} C_n = \bigcap_{n = 1}^\infty \bigcup_{m \geq n} C_m$, we have
    \[
        \P(\overline{\lim} C_n) \leq \P(\bigcup_{m \geq n} C_m) \leq \sum_{m \leq n} \P(C_m) \rightarrow 0 
    \]
\end{proof}

\begin{thm}\label{thm:4}
    If there exists a sequence of positive constants $ \{ \epsilon_n \}$ with $\sum_{n} \epsilon_n < \infty$ and \[
        \sum_{n} \P( |X_{n + 1} - X_n | > \epsilon_n) < \infty
    \] then $X_n$ converges almost surely to some limit $X$.
\end{thm}

\begin{proof}
    Let $A_n = \{ | X_{n+1} - X_n | > \epsilon_n$.  So from the above Lemma, $\P( \overline\lim A_n) = 0$.  We also have that $\omega \in \overline\lim A_n $ if and only if $\omega$ is in infinitely many $A_m$. For $\omega \notin \overline \lim A_n$, then there is a last set containing $\omega$.  Define $N(\omega) = n$ if $\omega \in \bigcup_{m \geq n} A_m - \bigcup_{m > n} A_m$, and zero if $\omega \in ( \bigcup{m \geq 1} A_m)^c$.
    
    For $\omega \notin \overline \lim A_n$, we have $\sum_{n = 1}^\infty X_{n+1}(\omega) - X_n(\omega)$ exists as $\sum_n \epsilon_n  < \infty$.  Since \[
        X_n(\omega) = X_1(\omega) + ( X_2(\omega) - X_1( \omega) ) + \dots + ( X_n( \omega) - X_{n-1}(\omega))
    \] we know $\lim X_n(\omega)$ exists - i.e. $\P( \lim X_n( \omega)) \text{ exists} ) = 1$.
\end{proof}

\begin{thm}Every sequence of random variables $X_n$ that converges almost surely converges in probability.  Conversely, if $X_n \cip X$ then there exists a subsequence $\{ X_{n_k} \}$ which converges almost surely.  
\end{thm}
\begin{proof}
    Assume $X_n \cas X$.  Let $\epsilon > 0$.  Consider $\overline \lim \P( |X_n - X | > \epsilon) \leq \P( \limsup \{ |X_n - X | > \epsilon \} )$ by a previous theorem (Theorem 2 in Lecture Notes).  We have \begin{align*}
        \limsup \{ | X_n - X | > \epsilon \} &= \{ \omega \, | \, | X_n(\omega) - X(\omega)| > \epsilon \text{ infinitely often} \} \\
                                            &\subseteq \{ \omega \, | \, \lim X_n (\omega) \neq X(\omega) \}
    \end{align*}
    Hence, we have \[
        \P( \overline \lim | X_n - X | > \epsilon ) \leq 1 - P (\lim X_n(\omega) = X(\omega)) = 0 \quad \text{as $X_n \cas X$}
    \] since $\lim \P( |X_n - X | > \epsilon) = 0$.
    
    Conversely, assume $X_n \cip X$.  Given $\epsilon > 0$, consider $\P(|X_n - X_m | > \epsilon) \leq \P(|X- X_m | > \epsilon/2 + \P( | X- X_n | > \epsilon/2 )$ (If $|X-X_n| \leq \epsilon/2$ and $|X-X_m| \leq \epsilon/2$, then $|X_n - X_m| \leq \epsilon$ by the triangle inequality). 
    Thus, $\P(|X_m - X_n | > \epsilon) \rightarrow 0$ as $m$ and $n \rightarrow 0$.  Set $n_1 = 1$ and define $n_j$ to be the smallest integer $N > n_{j-1}$ such that \[
        \P(|X_r - X_s| > 2^{-j} ) < 2^{-1} \quad \text{when $r, s > N$}
    \]
    Then apply Theorem \ref{thm:4}, and as \[
        \sum_j \P( |X_{n_{j+1}} - X_{n_j} | > 2^{-j}) < \sum 2^{-j} = 1 < \infty
    \] we know that $X_{n_j}$ converges almost surely.
    
\end{proof}
\begin{exmp}\label{exmp:cip_does_not_imply_as}
    We now construct an example where $X_n \cip 0$ but $X_n$ does not converge almost surely to $0$.  

    Let $\Omega = [0,1], \sigf$ the Borel $\sigma$-field, and $\P$ the Lebesgue measure.  Let $\phi_{kj} = \I_{[j-1/k, j/k]}$ for $j = 1, \dots, k$ and $k = 1, 2, \dots$.  

    Let $X_1 = \phi_{11}, X_2 = \phi_{21}, X_3 = \phi_{22}$, etc.  For any $p > 0$, \[
        \E(|X_n|^p) = \int X_n \, d\P = [ j_n - 1/k_n, j_n/k_n] \rightarrow 0
    \] and so $X_n \clp 0$.
    
    However, for each $\omega \in \Omega$ and each $k$ there are some $j$ such that $\phi_{kj}(\omega) = 1$.  Thus $X_n(\omega) = 1$ infinitely often. Similarly $X_n(\omega) = 0$ infinitely often.  Hence $X_n$ does not converge almost surely to 0.
\end{exmp}
% section lecture_4_thursday_10_march (end)

\section{Lecture 5 - Thursday 17 March} % (fold)
\label{sec:lecture_5_thursday_17_march}

Following from the previous lecture, we now modify the examples to show convergence in probability does not imply convergence in $L^p$ even when $\E(|X_n|^p) < \infty$.

From \ref{exmp:cip_does_not_imply_as}, replace $\phi_{kj}$ by $k^{1/p} \phi_{kj}$.  Then \[
    \P(|X_n| > 0) = 1/{k_n} \rightarrow 0
\] as $n \rightarrow \infty$.  Similar,y \[
    \E(|X_n|^p) = (k_n^{1/p})^p \P(X_n \neq 0) = 1
\] and so \[
    \lim_{n \rightarrow \infty} \E(|X_n|^p) = 1
\] and thus $X_n$ does not converge in $L^p$ to zero.  Thus convergence in probability does not imply convergence in $L^p$.

Next define $X_1 = \phi_{11}$, $X_n = \phi_{n1}n^{1/p}$.  Then \[
    X_n(\omega) \rightarrow 0
\] for $\omega > 0$ so $X_n \cas 0$.  We also have \[
    \E(|X_n|^p) = (n^{1/p})^p \frac{1}{n} = 1 
\] and so $X_n$ does not converge in $L^p$ to zero.

\begin{defn}[Uniform integrability]
    A sequence $\{ X_n \}$ is uniformly integrable if \[
        \lim_{y \rightarrow \infty} \sup_{n} \int_{|X_n| \geq y} |X_n | \, d\P = 0
    \] 
    
\end{defn}


\begin{thm}[Convergence in probability and uniform integrability imply convergence in $L^p$]
    If $X_n \cip X$ and $\{ | X_n | \}$ is uniformly integrable, then $X_n \clp X$.
\end{thm}

\begin{defn}[Independence]
    Let $(\Omega, \sigf, \P)$ be a probability space.  Let $A_1, A_2, \dots, A_n \in \sigf$  The events are said to be independent if \[
        \P(A_{i_1}, \dots, A_{i_k}) = \P(A_{i_1}) \dots \P(A_{i_k})
    \] for all $1 \leq i_1 < \dots < i_k \leq n$, $k = 2, 3, \dots, n$.
    
    In the infinite case, let $\{A_{\alpha}, \alpha \in I \}$, $I$ an index set, is a set of independent events if each finite subset is independent.
    
\end{defn}

\begin{defn}[Independence of random variables]
    Let $X_{1}, \dots, X_n$ be random variables on $(\Omega, \sigf, \P)$.  $X_1, \dots, X_n$ are independent if $A_{i} = \{ X_i \in S_i \}$ are independent for every set of Borel sets, $S_i \in \mathcal{B}$.
    
    Alternatively, let $X$ and $Y$ be random variables.  Let $\mathcal{B}_2$ be the Borel $\sigma$-field on $\R^2$.  $Z(\omega) = (X(\omega), Y(\omega))$ is then a map form $\Omega$ to $\R^2$.  $Z$ is Borel measurable if \[
        Z^{-1}(S) \in \sigf
    \] for all $S \in \mathcal{B}_2$. 
    $\P_{X,Y}$ is the induced measure on $B_2$, and $F_{X,Y}$ is the joint distribution of $(X,Y)$.  Let \[
        F_{X,Y}(x,y) = \P_{X, Y} ( (-\infty,x], (-\infty,y] ) = \P( \{ \omega: X(\omega) \leq x, Y(\omega) \leq y \} ) 
    \] 
\end{defn}

\begin{thm}
    If $X$ and $Y$ are independent then \[
        F_{X,Y}(x,y) = F_X(x) F_Y(y)
    \]
\end{thm}

\begin{thm}
    Let $X$ and $Y$ be independent, with $\E(|X|) < \infty$ and $\E(|Y) < \infty$.  Then \[
        \E(XY) = \E(X) \E(Y)
    \]
\end{thm}

\begin{proof}
    Start with simple functions.  Then \[
        X(\omega) = \sum_{i=1}^n a_i \mathbf{1}_{A_i} (\omega)
    \] with $\{ A_i \}$ disjoint.
    Let \[Y(\omega) = \sum_{j=1}^m b_j \mathbf{1}_{B_j}(\omega) 
    \] with $\{ B_j \}$ disjoint.  
    
    Independence implies $\P(A_i B_j) = \P(A_i) \P(B_j)$.
    
    Then \begin{align*}
        \E(XY) &= \sum_{i=1}^n \sum_{j=1}^m a_i b_j \E( \mathbf{1}_{A_i} \mathbf{1}_{B_j} ) \\
                &= \sum_{i=1}^n \sum_{j=1}^m a_i b_j \P(A_i B_j) \\
                &= \sum_{i=1}^n \sum_{j=1}^m a_i b_j \P(A_i) \P(B_j)
    \end{align*} by independence.

    Now extend to non-negative random variables $X,Y$ by constructing sequences of simple functions using monotone convergence theorem.  Let \[
        X_n(\omega) = \frac{i}{2^n} \quad \text{if } \frac{i}{2^n} < X(\omega) \leq \frac{i+1}{2^n}, i = 0,1,\dots, n 2^n 
    \]and zero if $X(\omega) > n$.
    
    For simple functions, we have \[
        \E(X_n Y_n) = \E(X_n) \E(Y_n)
    \] and so by the monotone convergence theorem, \[
        \E(XY) = \E(X) \E(Y)
    \]
\end{proof}

\begin{thm}
    Let $X$ and $Y$ be independent random variables.  Then \[
        \E(|X+Y|^r) < \infty
    \] if and only if \[
        \E(|X|^r) < \infty \text{ and } \E(|Y|^r) < \infty
    \] for any $r > 0$.
\end{thm}

\begin{lem}[$c_r$ inequality]
    We have \[
        |x+y|^r \leq c_r \left( |x|^r + |y|^r \right) 
    \] for $x,y$ real, $c_r$ constant, $r \geq 0$.
\end{lem}
\begin{proof}
    If $r = 0$, trivial.  
    
    If $r = 1$, we obtain the triangle inequality.
    
    If $r > 1$, we have \begin{align*}
        |x+y|^r &\leq [ 2 \max( |x|, |y|) ]^r \\
                &= 2^r \max(|x|^r, |y|^r) \\
                &\leq 2^r (|x|^r, |y|^r)
    \end{align*} and setting $c_r = 2^r$ proves for $r > 1$.
                    
    If $ 0 < r < 1$, consider $f(t) = 1 + t^r - (1+t)^r$, with $f(0) = 0$.  Differentiating, we have $f'(t) = rt^{r-1} - r(1+t)^{r-1} \geq 0$ for $t > 0$.  Thus $f(t)$ is increasing for $t >0$.  Hence \begin{align*}
        f(t) &> f(0) = 0 \\
        1+t^r &\geq (1+t)^r.
    \end{align*}  Using $t = \frac{|y|}{|x|}$, we obtain \[
        (|x| + |y|)^r \leq |x|^r + |y|^r
    \]
    
\end{proof}

% section lecture_5_thursday_17_march (end)

\section{Lecture 6 - Thursday 17 March} % (fold)
\label{sec:lecture_6_thursday_17_march}
\begin{lem}
    For any $\alpha > 0$ and distribution function $F$, \[
        \int_0^\infty x^{\alpha} \, dF(x) = \alpha \int_0^\infty x^{\alpha - 1} [ 1 - F(x)] \, dx
    \]
\end{lem}

\begin{proof}
    Consider.  Integrating by parts, we have that this is equal to 

\begin{align*}
    \int_0^b x^{\alpha} \, dF(x) &= -\int_0^b x^\alpha \, d(1-F(x)) \\
                            &= [-x^\alpha] (1-F(x))|_0^b + \int_0^b \alpha x^{\alpha - 1} (1 - F(x)) \, dx \\
                            &= -b^{\alpha}(1-F(b)) + \int_0^b \alpha x^{\alpha - 1} (1 - F(x)) \, dx  \\
\end{align*}

We also have \[
    0 \leq b^\alpha (1-F(b)) \leq \int_b^\infty x^\alpha \, dF(x)
\]
If the LHS converges then $\lim_{b \rightarrow \infty} \int_0^\infty x^\alpha \, dF(x) \rightarrow 0$.  Thus the term $b^\alpha (1-F(b))$ is squeezed to zero.

Conversely, \[
    \int_0^b x^\alpha \, dF(x) \leq \int_0^b \alpha x^{\alpha - 1} (1-F(x)) \, dx
\] and so \[
    \int_0^\infty \alpha x^{\alpha - 1} (1-F(x)) \, dx < \infty \Rightarrow \int_0^\infty x^\alpha \, dF(x) < \infty.
\]
\end{proof}

\begin{thm}
    Let $X,Y$ independent and $r > 0$. Then \[
        \E( |X+Y|^r) <\infty  \iff \E(|X|^r) \infty, \E(|Y|^r) < \infty
    \] 
\end{thm}

\begin{proof}
    If $\E(|X|^r) < \infty$, $\E(|Y|^r) < \infty$.  Then \[
        \E(|X+Y|^r) \leq c_r (\E(|X|^r) + \E(|Y|)^r ) < \infty
    \]
    
    Assume $\E(|X+Y|^r) < \infty$.  Assume $X$ and $Y$ have median 0 (without loss of generality).  Then \[
        \P(X \leq 0) \geq \frac{1}{2}, \P(X \geq 0) \geq \frac{1}{2}
    \] Similarly for $Y$. 
    
    Now, \begin{align*}
        \P(|X| > t) &= P(X < -t) + P( X > t), t > 0 \\
                    &= \frac{P(X  < -t, Y \leq 0)}{P(Y \leq 0)} + \frac{P(X > t, Y \geq 0)}{P(Y \geq 0)} \\
                    &= 2 P(X+Y \leq -t) + 2 P(X+Y > t) \\
                    &= 2 P(|X+Y| > t)
    \end{align*} by independence.
    
    Using the previous lemma, we have \begin{align*}
        \E(|X|^r) & \int_0^\infty x^r \, dF(x) = r \int_0^\infty x^{r-1} P(|X| > x) \, dx \\
                            &\leq 2r \int_0^\infty x^{r-1} P(|X+Y| > x) \, dx \\
                            = 2r \E(|X+Y|^r).
    \end{align*}  So $\E(|X+Y|^r) < \infty \Rightarrow \E(|X|^r) < \infty$.  Similarly for $\E(|Y|^r) < \infty$.  

\end{proof}

\begin{thm}
    If $X$ and $Y$ are independent with distribution functions $F$ and $G$ respectively, then 
    \begin{align*}
        P(X+Y \leq x) &= \int_\R F(x-y) \, dG(y) \\
                      &= \int_\R G(x-y) \, dF(y)
    \end{align*}
\end{thm}
\begin{proof}
    This is just a simple statement of Fubini's theorem.
\end{proof}

\begin{cor}
    Suppose that $X$ has an absolutely continuous distribution function \[
        F(x) = \int_{-\infty}^x f(u) \, du
    \] for some density function $f$ with $\int_\R f(x) \, dx = 1$ and $f \geq 0$.  
    
    Let $Y$ be independent of $X$.  Then $X+Y$ has an absolutely continuous distribution with density \[
        \int_\R f(x-y) \, dG(y)
    \]
    
    Thus we have
    \begin{align*}
        P(X+Y \leq x)   &= \int_\R \int_{-\infty}^x f(t-y) \, dt \, dG(y) \\
                        &= \int_{-\infty}^x \int_\R f(t-y) \, dG(y) \, dt
    \end{align*}
\end{cor}

\begin{defn}
    A distribution function $F$ that can be represented in the form \[
        F(x) = \sum_j b_j \mathbf{1}_{[a_j, \infty]}(x)
    \] with $a_j$ real, $b_j \geq 0$, $\sum_{b_j} = 1$ is called \textbf{discrete}.
    
\end{defn}

If a distribution function is continuous then it may be:
\begin{enumerate}
    \item \textbf{Absolutely continuous}, in which case there is a density function $f \geq 0$ such that $F(b) - F(a) = \int_a^b f(u)\, du$.  $f$ is called the density.
    \item \textbf{Singular}, in which case $F'(x)$ exists and equal zero almost everywhere with respect to the Lebesgue measure (see Chung \S1.3)
\end{enumerate}

\begin{thm}
    Any distribution function $F$ can be written uniquely as a convex combination of a discrete, an absolutely continuous, and a singular distribution.  By convex, we mean a linear combination with non-negative coefficients summing to one.   
\end{thm}

\begin{thm}[Chebyshev's inequality]
    Let $X$ be a random variable and $g$ an increasing, non-negative function.  If $g(a) > 0$, then $$ P(X \geq a) \leq \frac{\E(g(X))}{g(a)}.$$
\end{thm}

\begin{proof}
    We have  \begin{align*}
        \E(g(X)) &= \int_\R g(x) \, dF(x) \\
                &\geq \int_a^\infty g(x) \, dF(x) \\
                &\geq g(a) \int_a^\infty dF(x) \\
                &= g(a) P(X \geq a) 
    \end{align*}
\end{proof}

\begin{cor}
    Let $g(x) = x^2$.  Then\[
        P( | X - \E(X) | > a) \leq \frac{\text{Var}(X)}{a^2}
    \]

    Let $g(x) = e^{ax}$.  Then \[
        P(X \geq a) \leq \frac{\E(e^{cX})}{e^{ca}} = e^{-ca} \E(e^{cX})
    \]
    
    Let $g(x) = |x|^k, k > 0$. Then \[
        P(|X| \geq a) \leq \frac{\E(|X|^k)}{a^k}.
    \]
\end{cor}
% section lecture_6_thursday_17_march (end)

\section{Lecture 7 - Thursday 24 March} % (fold)
\label{sec:lecture_7_thursday_24_march}
\begin{defn}[Weak law of large numbers]
    Let $X_1, \dots, X_n \dots$ be IID random variables with $\E(X_i) = \mu$,$ \var(X_i) = \sigma^2 < \infty$.  Then \[
        \overline{X}_n \cip X
    \]
\end{defn}

\begin{proof}
    \begin{align*}
        P(|\overline{X}_n) &\leq \frac{\E(\overline{X}_n - \mu)^2}{\epsilon^2} \\
                            &= \frac{\sigma^2/n}{\epsilon^2} \rightarrow 0
    \end{align*} as $n \rightarrow \infty$.  
    
    We have \[
        \E(|\overline{X}_n - \mu|^2) = \sigma^2/n \rightarrow 0
    \] and so $\overline{X}_n$ converges to $\mu$ in $L^2$
\end{proof}

We can relax the assumptions to $E(|X) < \infty$ (no need to have finite variance).  See Chung (1974) p.109, Theorem 5.2.2.

\begin{thm}
    Let $X_i$ be uncorrelated, and $\E(X_i) = \mu_i$, $\var(X_i) = \sigma^2_i < \infty$ with \[
        \frac{1}{n^2} \sum_{i=1}^n \sigma^2_i \rightarrow 0
    \]  then we have \[
        \overline{X}_n - \frac{1}{n} \sum_{i=1}^n \mu_i \cip 0
    \]
\end{thm}
\begin{proof}
    \begin{align*} P(|\overline{X}_n - \frac{1}{n} \sum_{i=1}^n \mu_i | > \epsilon) &= P( \frac{1}{n} \sum_{i=1}^n (X_i - \mu_i)| > \epsilon) \\
        &\leq \frac{\var(\frac{1}{n}\sum_{i=1}^n (X_i - \mu_i))}{\epsilon^2} \rightarrow 0      
    \end{align*} as $\sum_{1}{n^2} \sum_{i=1}^n \sigma_i^2 \rightarrow 0$.  
\end{proof} 


\begin{thm}[Borel-Cantelli lemma]
    Let $A_1, \dots$ be events in a probability space.  Let $B = \limsup A_n$ = $\bigcap_{n \geq 1} \bigcup_{m \geq n} A_m$.  Then 
    \begin{enumerate}[(i)]
        \item $\sum_{n} P(A_n) < \infty$ then $P(B) = 0$.  
        \item If $A_i$ are independent and $\sum_n P(A_n) \rightarrow \infty$ then $P(B) = 1$.
    \end{enumerate}
    For (ii) we need independence. Consider $A_i = A$ where $P(A) = \frac{1}{3}$.  Then \[
        B = \limsup A_n = A
    \] and $P(B) = \frac{1}{3}$
\end{thm}

\begin{proof}
    Preliminary lemma - if $0 < x < 1$, then $\log(1-x) < -x$.  We can then show that if $\sum_n a_n \rightarrow \infty$ then $\prod_n (1- a_n) \rightarrow 0$.  
    
    \begin{enumerate}[(i)]
        \item \[ P(B) \leq P(\bigcup_{m \geq n} A_n) \leq \sum_{m  \geq n} P(A_m) \rightarrow 0 \] and so $P(B) = 0$.
        \item We will prove $P(\bigcup_{m \geq n} A_m) = 1$ for all $n$. Take $K > n$.  Then \begin{align*}
            1 - P(\bigcup_{m \geq n} A_m) &\leq 1 - P(\bigcup_{m=n}^K A_m) \\
                                        &= P( (\bigcup_{m=n}^K A_n)^c) \\
                                        &= P( \bigcup_{m=n}^K A_m^c) \\
                                        &= \prod_{m=n}^K ( 1 - P(A_m)) \quad \text{by independence} \\
                                        &\rightarrow 0
        \end{align*}  as $\sum_{n} P(A_n) \rightarrow \infty$ as $K \rightarrow \infty$.  Thus \[
            P(\bigcup_{m \geq n} A_m) = 1
        \] for all $m$, and so $P(B) = 1$.  
    \end{enumerate}
\end{proof}

\begin{thm}[Strong law of large numbers]  Let $X_1, \dots$ be IID random variables.  Let $\E(X_1) = \mu$, $\E(X_1^4) < \infty$.  Let $S_n = \sum_{j=1}^n X_j$.  Then \[
    \overline X_n = \frac{1}{n} S_n \cas \mu 
\]
\end{thm}

\begin{proof}
    \begin{align*}
        \E(\sum_{i=1}^n (X_i - \mu))^4 &= \sum_{i=1}^n E(X_i - \mu)^4 + 6 {n \choose 2} \sigma^4 \\
        &= n \E(X_1 - \mu)^4 + 3n(n-1) \sigma^4 \\
        &\leq Cn^2. 
    \end{align*}
    From Chebyshev, we have 
    \begin{align*}
        P(| S_n - \mu n| > \epsilon n) &\leq \frac{E(S_n - \mu n)^4}{)(\epsilon n)^4} \\
                &\leq \frac{cn^2}{\epsilon^4 n^4} = \frac{k}{n^2} 
    \end{align*}
    and so \[
        \sum_n P(|S_n - n \mu| > n \epsilon) < \infty,  \]
    and so $P(\limsup \{ | \frac{S_n}{n} - \mu | > \epsilon \} ) = 0$.  Letting $A_\epsilon = \{ | \frac{S_n}{n} - \mu | > \epsilon \}$.  Then 
    \begin{align*}
        P(|\frac{S_n}{n} - \mu | \text{ does not converge to zero}) &= P(\bigcup_k A_{1/k}) \\
        &\leq \sum_{k} P(A_{1/k}) \\
        &= 0
    \end{align*} by Borel-Cantelli.
\end{proof}
% section lecture_7_thursday_24_march (end)

\section{Lecture 8 - Thursday 24 March} % (fold)
\label{sec:lecture_8_thursday_24_march}
    Let $X_1, \dots$ be IID random variables with mean $\mu$.  Then \[
        P(\lim_{n \rightarrow \infty} \frac{S_n}{n} = \mu) = 1
    \]
    Conversely, if $\E(|X|)$ does not exist, then \[
        P( \limsup |\frac{S_n}{n}| = \infty) = 1
    \]

\begin{thm}
    If $E(X^2) < \infty$, and $\mu = 0$ (WLOG), 
    \begin{align*}
        P( |n^{-\alpha} S_n | \geq \epsilon) \leq \frac{E(S_n^2)}{n^{2 \alpha} \epsilon^2} \\
        &=n ^{1-2\alpha} \sigma^2/\epsilon^2 \rightarrow 0
    \end{align*} provided $S \geq \frac{1}{2}, n^{-\alpha} S_n \cip 0$.  
\end{thm}

\begin{thm}[Hausdorff (1913)]   $|S_n | = \mathcal{O}(n^{\frac{1}{2} + \epsilon})$ a.s for any $\epsilon > 0$.
    
    Assumes $\E(|X_i|^r) < \infty$ for $r = 1, 2, \dots$. 
\end{thm}

\begin{proof}
    Previously, we showed $\E(S_n^4) \leq Cn^2$ for some $C > 0$.  Then we can extend this to \[
        \E(S_n^{2k}) \leq c_k n^k, k = 1, 2, \dots
    \] Then \begin{align*}
        P(n^{-\alpha} |S_n| > a ) &\geq \frac{c_k n^k}{(a n^{\alpha})^{2k}} \\
                                &= c_k a^{-2k} n^{k(1-2\alpha)}
    \end{align*} and so \[
        \sum P( n^{-\alpha} |S_n| > a) < \infty 
    \] if $k(1-2\alpha) > -1$ i.e. $\alpha \geq \frac{1}{2} + \frac{1}{2k}$.  
    
    By Borel-Cantelli, $P(|S_n| > an^{\alpha} \, i.o.) = 0$ if $\alpha > \frac{1}{2} + \frac{1}{2k}$.  
\end{proof} 
\begin{thm}[Hardy and Littlewood (1914)] $|S_n| = \mathcal{O}(\sqrt{n \log n})$ a.s.
\end{thm}
\begin{lem}
    Suppose $|X_i| \leq M$ a.s. ($X_i$ is bounded).  Then for any $x \in [0, \frac{2}{M}]$, we have \[
        \E(e^{x S_n}) \leq \exp[ \frac{nx^2 \sigma^2}{2}(1 + xM)]
    \]  
\end{lem}

\begin{proof}
    The random variables $e^{x X_i}$ are independent, so $\E(e^{x S_n}) = \left[ \E(e^{x X_1})\right]^n$.  We can then evaluate 
    \begin{align*}
        \E(e^{x X_1}) &= \E \left[ \sum_{k=0}^\infty \frac{(x X_1)^k}{k!} \right] \\
        &= 1 +0 + x^2 \sigma^2/2 + \E(\sum_{k=3}^\infty \frac{(x X_1)^k}{k!}) \\
        &\leq 1 + x^2 \sigma^2/2 + \sum_{k=3}^\infty \frac{x^k M^{k-2} \sigma^2}{k!} \\
        & \leq 1  + x^2 \sigma^2/2 + \sigma^2 M^{-2}/3! \sum_{k=3}^\infty \frac{x^k M^k}{3^{k-3}} \\
        &= 1 + x^2 \sigma^2/2 + \sigma^2 M^{-2}/6  \frac{ (xM/3)^3}{(1-xM/3)} \\
        &= 1 + x^2 \sigma^2/2 = \frac{\sigma^2 M x^3}{6(1-xM/3)}.
    \end{align*}  If $0 \leq x \leq 2/M$, we have \begin{align*}    
    \E(e^{x X_1})   &\leq 1 + \sigma^2 x^2/2 + \sigma^2 x^2 /2(xM) \\
                        &= 1 + \sigma^2 x^2/2(1 + xM) \\
                        &\leq \exp(\sigma^2 x^2/2(1 + xM))
    \end{align*}

\end{proof}

\begin{cor}
    For $0 < a < \frac{2\sigma^2 n }{M}$, under the conditions of the above Lemma, \[
        P(S_n \geq a) \leq e^{-\frac{a^2}{2n\sigma^2})1 - \frac{Ma}{n\sigma^2}}
    \]
\end{cor}
\begin{proof}
    \begin{align*}
        P(S_n \geq a) &\leq \frac{E(e^{xS_n})}{e^{ax}} \\
                    &\leq \exp( \frac{n \sigma^2 x^2}{2}(1 + xM) - ax) \quad 0 < x \leq \frac{2}{M}
    \end{align*} Put $x = \frac{a}{n \sigma^2}$.  THen 
    \begin{align*}
        P(S_n \geq a) &\leq \exp( \frac{a^2}{2 n \sigma^2}(1 = \frac{aM}{n \sigma^2}) - \frac{a^2}{n \sigma^2}) \\
        &= \exp( \frac{-a^2}{2n \sigma^2}(1 - \frac{aM}{n \sigma^2})) 
    \end{align*}
\end{proof}

We can now prove the Hardy-Littlewood result.  
If $|X_i| \leq M$ almost surely then $|S_n| = \mathcal{O}(\sqrt{n \log n})$ a.s.
\begin{proof}
    Put $a = c \sqrt{ n \log n}$.  Then 
    \begin{align*}
        P(S_n \geq c \sqrt{n \log n}) &\leq \exp( \frac{c^2 \log n}{2 \sigma^2}( 1 - \frac{Mc \sqrt{\log n}}{\sqrt{n} \sigma^2})) \\
        &= n^{-c^2/2\sigma^2} \exp( \frac{Mc^3}{2\sigma^4} \frac{\log n \sqrt{\log n}}{\sqrt{n}}) 
    \end{align*}
    If $c^2 > 2 \sigma^2$ then $\sum_{n} P(S_n > c \sqrt{n \log n}) < \infty$.  By Borel-Cantelli, we then have \[
        P(S_n > c \sqrt{n \log n} \, i.o.) = 0
    \] Now apply the argument to $-X_i$.  Then \[
        P(-S_n > c \sqrt{n \log n} \, i.o.) = 0
    \]
\end{proof}

\begin{thm}[Khintchine (1923)] $|S_n | = \mathcal{O}(\sqrt{ n \log \log n})$ a.s.
\end{thm}
\begin{thm}[Khintchine (1924)]Let $X_i = \pm 1$ with probability $\frac{1}{2}$.  Then \[
    \limsup \frac{|S_n|}{\sqrt{n \log \log n}} = \sqrt{2} a.s.
\]
\end{thm}


% section lecture_8_thursday_24_march (end)

\section{Lecture 9 - Thursday 31 March} % (fold)
\label{sec:lecture_9_thursdy_31_march}
\begin{defn}[Induced $\sigma$-field]
    Let $(\Omega, \sigf, \P)$ be a probability space.  Let $Y$ be a set of random variables on $(\Omega, \sigf)$.  Then $\sigma(Y)$ is the smallest $\sigma$-field contained in $\sigf$ with respect to which each $X \in Y$ is measurable.  
    
    That is, for each $B \in \mathcal{B}$, the Borel $\sigma$-field on $\R$, we have \[
        X^{-1}(B) \in \sigma(Y)
    \]  Thus $\sigma(Y)$ is the intersection of all $\sigma$-fields which contain every set of the form $X^{-1}(B)$ for all $B \in \mathcal{B}, X \in Y$.  
\end{defn}

\begin{defn}[Independent $\sigma$-fields]
    If $X_1, \dots$ are independent random variables and $A_i \in \sigma(X_i)$, then \[
        P(\bigcap_{i=1}^n A_i) = \prod_{i=1}^n P(A_i) \tag{$\star$}
    \]
    If $\sigf_1, \sigf_2, \dots$ are $\sigma$-fields contained in $\sigf$ and $(\star)$ holds for any $A_i \in \sigf_i$ then we sa the $\sigma$-fields are independent. 
\end{defn}

\begin{thm}
    Let $\sigf_0, \sigf_1, \dots$ be independent $\sigma$-fields and let $\mathcal{G}$ be $\sigma$-fields generated by any subset of $\sigf_1, \sigf_2, \dots$.  Then $\sigf_0$ is independent of $\mathcal{G}$.  
\end{thm}

\begin{proof}
    \textbf{Outline.} Take $\mathcal{G}$ to be the smallest $\sigma$-field containing $\sigf_1, \sigf_2, \dots$.  
    
    If $A \in \sigf_0$, $B \in \mathcal{G}$, then we need to show \[
        P(A \cap B) = P(A) P(B).
    \]
    \begin{enumerate}[(1)]
        \item Assume $P(A) > 0$. 
        \item If $B = A_1 \cap A_2 \dots A_n$ then the result is true.
        \item Let $\mathcal{G}_a$ be the class of \textbf{finite} unions of $B$.  Then $\mathcal{G}_a$ is a finitely additive field, and $G \in \mathcal{G}_a$ can be written as $G = \bigcup_{i=1}^k G_i$ where $G_i$ has the form of $B$ above.  Then 
        \begin{align*}
            P(A \cap G) &= P( \bigcup_{i=1}^k A \cap G_i) \\
                        &= \sum P(A \cap G_i) = \sum_{i=j} P(A \cap G_i \cap G_j) + \dots \\
                        &= P(A) P(G)
        \end{align*} by the inclusion-exclusion formula and independence of $A$ and $G_i$.  
        \item Now, let $P_A(B) = \frac{P(A\cap B)}{P(A)}$.  Then $P_A$ and $P$ are measures on $\sigf$, and $P$ and $P_A$ agree on $\mathcal{G}_a$.  Thus by the extension theorem they agree on the $\sigma$-field generated by $\mathcal{G}_a$ which includes $\mathcal{G}$.
    \end{enumerate}
\end{proof}

\begin{defn}[Tail $\sigma$-field]
    Let $X_1, X_2, \dots$ be a sequence of random variables and let \[
        \sigf_n = \sigma(\{ X_n , X_{n+1}, \dots \})
    \] be the $\sigma$ field generated by $X_n, X_{n+1}$.  Then \[
        \sigf_n \supseteq F_{n+1} \supseteq F_{n+2} \dots
    \] and let \[
        \mathcal{T} = \bigcap{n} \sigf_n
    \] be the \textbf{tail $\sigma$-field}.
    
    $\mathcal{T}$ is the collection of events defined in terms of $X_1, X_2, \dots$ not affected by altering a finite number of the random variables.  

\end{defn}

\begin{thm}[The $0-1$ law]
    Any set belonging to the tail $\sigma$-field of a sequence of independent random variables has probability $0$ or $1$.      
\end{thm}   
\begin{proof}
    We have $\sigma(X_n)$ is independent of $\sigma( \{ X_{n+1}, X_{n+2}, \dots \}) = \sigf_{n+1} \supseteq \mathcal{T}$ and so $\mathcal{T}$ is independent of $\sigma(X_n)$ for every $n$.  By the previous theorem, it follows that $\sigf$ is independent of $\mathcal{G} = \sigma(\{ X_1, X_2, \dots \})$ but as $\mathcal{T} \subseteq \mathcal{G}$, we know that $\mathcal{T}$ is independent of itself.  Thus, for any $A \in \mathcal{T}$, \[
        P(A \cap A) = P(A) P(A)
    \] and so $P(A) = 0$ or $1$.  
\end{proof}

\subsection{Martingales} % (fold)
\label{sub:martingales}

% subsection martingales (end)
\begin{defn}[Martingale]
    Let $(\Omega, \sigf, \P)$ be a probability space.  Let $\{ \sigf_n \}$ be an increasing sequence of $\sigma$-fields.  \[
        \sigf_n \subseteq \sigf_{n+1} \subseteq \dots \subseteq \sigf. 
    \]  Let $\{ S_n \}$ be a sequence of random variables on $\Omega$.  Then $\{ S_n \}$ is a \textbf{martingale} with respect to $\{ \sigf_n \}$ if 
    \begin{enumerate}[(1)]
        \item $S_n$ is measurable with respect to $\sigf_n$. 
        \item $\E(|S_n|) < \infty$.
        \item $\E(S_n \, | \, \sigf_m) = S_m$ almost surely for all $m \leq n$. 
    \end{enumerate}
\end{defn}

% section lecture_9_thursdy_31_march (end)

\section{Lecture 10 - Thursday 31 March} % (fold)
\label{sec:lecture_10_thursday_31_march}
\begin{defn}[Supermartingale]
$\{ S_n \}$ is a \textbf{supermartingale} with respect to $\{ \sigf_n \}$ if 
    \begin{enumerate}[(1)]
        \item $S_n$ is measurable with respect to $\sigf_n$. 
        \item $\E(|S_n|) < \infty$.
        \item $\E(S_n \, | \, \sigf_m) \leq S_m$ almost surely for all $m \leq n$. 
    \end{enumerate}
\end{defn}

\begin{defn}[Submartingale]
$\{ S_n \}$ is a \textbf{submartingale} with respect to $\{ \sigf_n \}$ if 
    \begin{enumerate}[(1)]
        \item $S_n$ is measurable with respect to $\sigf_n$. 
        \item $\E(|S_n|) < \infty$.
        \item $\E(S_n \, | \, \sigf_m) \geq S_m$ almost surely for all $m \leq n$. 
    \end{enumerate}
\end{defn}

\begin{defn}[Regular martingale]
    Let $X$ is a random variable $\E(|X|) < \infty$, $S_n = \E(X \, | \, \sigf_n)$ and assume $\{ S_n \}$ is a martingale with respect to $\{ F_n \}$.  
    
    If a martingale can be written in this way for some $X$ then it is \textbf{regular}.  
\end{defn}

Not every martingale is a regular martingale.  

\begin{exmp}
    Assume $P(X_i = 1) = p$, $P(X_i = -1) = 1-p$, and let $S_n = \sum_{i=1}^n X_i$.  If $p \neq \frac{1}{2}$ then \[
        Y_n = \left( \frac{1-p}{p} \right)^{S_n}
    \] is a martingale with respect to $\sigf_n = \sigma(X_1, \dots,X_n)$, since \begin{align*}
        \E(Y_n \, | \, \sigf_{n-1}) &= \E\left(\left( \frac{1-p}{p} \right)^{S_n + X_n} \, | \, \sigf_{n-1} \right) \\
        &= \left( \frac{1-p}{p} \right)^{S_n} \left[\left( \frac{1-p}{p} \right) p + \left(\frac{1-p}{p} \right)^{-1}(1-p) \right] \\
        &= Y_{n-1}
    \end{align*}
\end{exmp}

\subsection{Conditional expectations} % (fold)
\label{sub:conditional_expectations}
If $\sigg \subseteq \sigf$ then \[
    L^2(\sigg) = \{ X \, | \, \E(X^2) < \infty, \text{$X$ is $\sigg$-measurable} \}
\]

If $Y \in L^2$ define $Z = \E(Y \given \sigg)$ to be the projection of $Y$ onto $L^2(\sigg)$, where \[
    \E(Y-Z)^2 = \inf_{U \in L^2(\sigg)} \E(Y - U)^2
\] Then $Y-Z$ will be orthogonal to the elements of $L^2(\sigg)$.  That is, \begin{align*}
    \int (Y-Z) X \, dP = 0 
\end{align*} for all $X \in L^2(\sigg)$.  If $A \in \sigg$, then letting $X = \mathbf{1}_{A}$, we have \[ \boxed{
    \int_A Y \, dP = \int_A \E(Y \given \sigg) \, dP
}
\]

If $Y \geq 0$ construct $\{ Y_n \}$ with $Y_n \in L^2$ such that $Y_n \uparrow Y$.  Define \[
    \E(Y \given \sigg) = \lim_{n \rightarrow \infty} \E(Y_n \given \sigg).
\] The limit exists as \[
    \E(Y_n \given \sigg) \geq \E(Y_m \given \sigg), n \geq m.
\]  We still have 
\begin{enumerate}[(1)]

    \item $\E(Y \given \sigg)$ is $\sigg$-measurable, and 
    \item For all $A \in \sigg$, \[
        \int_A Y \, dP = \int_A \E(Y \given \sigg) \, dP
    \] as \[
        \int_A \E(Y \given \sigg) \, dP= \lim_{n \rightarrow \infty} \int_A \E(Y_n \given \sigg) \, dP = \lim_{n \rightarrow \infty} \int_A Y_n \, dP  = \int_A Y \, dP
    \] by the monotone convergence theorem.  
\end{enumerate}

If $Y \in L^1$, defining $Y = Y^+ - Y^-$, we define \[
    \E(Y \given \sigg) = \E(Y^+ \given \sigg) - \E(Y^- \given \sigg).  
\]
% subsection conditional_expectations (end)

\subsection{Stopping times} % (fold)
\label{sub:stopping_times}

\begin{defn}
    A map \[
        \nu : \Omega \rightarrow \overline{\N} = \{ 0, 1, 2, \dots, \infty \}
    \] is called a \textbf{stopping time} with respect to $\{ \sigf_n \}$, an increasing sequence of $\sigma$-fields, if \[
        \{ \nu = n \} \in \sigf_n.
    \] and thus \[
        \{ \nu \leq n \}, \{ \nu > n \} \in \sigf_n
    \]
\end{defn}

\begin{thm}[Properties of stopping times]
    Let $\sigf_\infty = \vee_{n=1}^\infty \sigf_n$, the $\sigma$-field generated by all $\sigf_n$.   Then we have 
    \begin{enumerate}[(1)]
        \item For all stopping times $\nu$, $\nu$ is $\sigf_\infty$-measurable.\[
            \{ \nu = n \} \in \sigf_n, \{ \nu = \infty \} = \{ \bigcup_{n} \{ v = n \} \}^c \in \sigf_\infty
        \]
        \item The minimum and maximum of a countable sequence of stopping times is a stopping time.  To prove this, let $\{ v_k\}$ be a sequence of stopping times.  Then \begin{align*}
        \{ \max_{k} v_k \leq n \} &= \bigcap_k \{ v_k \leq n \} \in \sigf_n \\
        \{ \min_{k} v_k > n \} &= \bigcap_k \{ v_k > n \} \in \sigf_n                           
        \end{align*}
    \end{enumerate} 
\end{thm}

\begin{lem}
    Let $\{ Y_n^1 \}$ and $\{ Y_n^2 \}$ be two positive supermartingales with respect to $\{ \sigf_n \}$, an increasing sequence of $\sigma$-fields.  Let $\nu$ be a stopping time.  If $Y_n^1 \geq Y_n^2$ on $[\nu = n]$, then \[
        Z_n = Y_n^1 \indic{\nu > n} + Y_n^2 \indic{\nu \leq n}
    \] is a positive supermartingale.  
\end{lem}


\begin{proof}
    We have that $Z_n$ is $\sigf_n$-measurable and positive.  We then have \begin{align*}
        \E(Z_n \given \sigf_{n-1}) &= \E( Y_n^1 \indic{\nu > n} + Y_n^2 \indic{\nu \leq n} \given \sigf_{n-1}) \\
        &= \E( Y_n^1 \indic{\nu > n-1} - Y_n^1 \indic{\nu = n} + Y_n^2 \indic{\nu \leq n-1} + Y_n^2 \indic{\nu = n}\given \sigf_{n-1}) \\
        &\leq Y_n^1 \indic{\nu > n-1} + Y_n^2 \indic{\nu \leq n-1} + \E((Y_n^2 - Y_n^1) \indic{\nu = n} \given \sigf_{n-1}) \\
        &\leq Z_{n-1} 
    \end{align*} as $Y_n^2 - Y_n^1 < 0$ on $\{ \nu = n \}$.  
\end{proof}
% subsection stopping_times (end)
% section lecture_10_thursday_31_march (end)

\section{Lecture 11 - Thursday 7 April} % (fold)
\label{sec:lecture_11_thursday_7_april}

\begin{thm}[Maximal inequality for positive supermartingales]
    Let $\{ Y_n \}$ be a positive supermartingale with respect to $\{ \sigf_n \}$.  Then \[
        \sup_{n } Y_n < \infty a.s
    \] on $[Y_0 < \infty]$ and \[
        P(\sup_{n} Y_n > a \, | \, \sigf_0 ) \leq \min(1, \frac{Y_0}{a})
    \]
\end{thm}
\begin{proof}
    Fix $a > 0$ and let $\nu_a= \inf \{ n : Y_n > a \} = \infty$ if $\sup_n Y_n \leq a$.  Then the sequence $Y_n(2) = a$ is a positive supermartingale, and so \[
        Z_n = Y_n \indic{\nu_a> n} + a \indic{\nu_a\leq n}
    \] is a positive supermartingale by the previous lemma.  Then we have \[
        \E(Z_n \, | \, \sigf_0) \leq Z_0 = \begin{cases}
            Y_0 & Y_0 \leq a \\
            a  & Y_0 > a
        \end{cases}
    \]  Thus $Z_n \geq a \indic{\nu_a\leq n}$ and so \[
        a P({va \leq n \given \sigf_0}) \leq \min(Y_0, a) 
    \] for all $a$.  THus \[
        P(\sup_{n} Y_n > a \given \sigf_0 ) = P(\nu_a< \infty \given \sigf_0) \leq \min(1, \frac{Y_0}{a})
    \]
\end{proof}

Write \begin{align*}
    P(Y_0 < \infty, \sup_{n} Y_n > a) &= \E(\indic{Y_0 < \infty} \indic{\sup_n Y_n > a}) \\
                                    &= \E(\indic{Y_0 < \infty}) \E(\indic{\sup_n Y_n > a} \given \sigf_0) \\
                                    &\leq \int_{Y_0 < \infty} \min(1, \frac{Y_0}{a}) \, dP \\
                                    &\rightarrow 0
\end{align*} as $a \rightarrow \infty$ by the dominated convergence theorem.  

Thus, we have \[
    P(Y_0 < \infty, \sup_{n} Y_n < \infty) = 1 a.s.
\]

Fix $a < b \in \R$.  For any process $Y_n$, define the following random variables \begin{align*}
    \nu_1 &= \min(n \geq 0, Y_n \leq a) \\
    \nu_2 &= \min(n > \nu_1, Y_n \geq b) \\
    \nu_3 &= \min(n > \nu_2, Y_n \leq a)
\end{align*} and so on.  If any $v_i$ is undefined it is subsequently set to infinity.

Define $\beta_{ab} = \max\ { p : \nu v_{2p} < \infty}$, equal to the number of upcrossings of $(a,b)$ by $Y_n$.  We have $\beta_{ab} = \infty$ if and only if $\liminf y_n \leq a < b \leq \limsup y_n$.  We also have $Y_n$ converges if and only if $\beta_{ab} < \infty$ for all rationals $a,b$, $a < b$.  

\begin{thm}[Dubin's inequality] If $Y_n$ is a positive supermartingale, then $\beta_{ab}(\omega)$ are random variables and for each integer $k \geq 1$, we have \[
    P(\beta_{ab} \geq k \given \sigf_0) \leq (\frac{a}{b})^k \min(1, \frac{Y_0}{a}), 0 < a < b.
\]  
\end{thm} 
\begin{proof}
    The $v_k$ defined above are stopping times with respect to $\sigf_n$, as \[ 
        [\nu_{2p} = n] = \bigcup_{m=0}^{n-1} [\nu_{2p-1} = m, Y_{m+1} \leq b, \dots, Y_{n-1} < b, Y_n \geq b ]
\] and as $\nu_1$ is a stopping time, we then use induction.   

    We then have $[\beta_{ab} \geq k] = [\nu_{2k} < \infty]$.  Then define \begin{align*}
        Z_n &= \indic{0 \leq n < \nu_1} + \sum_{k=1}^K (\frac{b}{a})^{k-1} \frac{Y_n}{a} \indic{\nu_{2k-1} \leq n \leq \nu_{2k}} \\
            &\qquad + \left(\frac{b}{a}\right)^k \indic{\nu_{2k} \leq n < \nu_{2k+1}} + \left( \frac{b}{a} \right)^K \indic{n \geq \nu_{2K+1}}
    \end{align*} 
    i.e. $\indic{0 \leq n < \nu_1} + \frac{Y_n}{a} \indic_{\nu_1 \leq n < \nu_2} + \frac{b}{a} \indic{\nu_2 \leq n < \nu_3} + \dots + \left( \frac{b}{a} \right)^K \indic{\nu_{2K} \leq n}$. 
    
    We now apply the previous lemma to show $\{ Z_n \}$ is a positive supermartingale.  We have \[
        \left( \frac{b}{a} \right)^k, \left( \frac{b}{a} \right)^{k-1} \frac{Y_n}{a}
    \] are positive supermartingales.  On $[ \nu_1 = n]$, we have $1 \geq \frac{Y_n}{a}$.  On $[\nu_{2k-1} = n]$ we have \[
        \left(\frac{b}{a} \right)^{k-1} \geq \left(\frac{b}{a} \right)^{k-1} \frac{Y_n}{a}
    \]
    
    On the even stopping times, we have $\left(\frac{b}{a} \right)^{k-1} \frac{Y_n}{a} \geq \left(\frac{b}{a} \right)^{k}$.  Thus \[
        \E(Z_n \given \sigf_0) \leq Z_0
    \] as $Z_n$ is a positive supermartingale. d
    
    Since $Z_n \geq \frac{b}{a}^K \indic{\nu_{2k} \leq n}$, we have \[
        P(\nu_{2k} \leq n \, | \, \sigf_0) \leq \frac{a}{b}^K \min(1, \frac{Y_0}{a}) 
    \]  Letting $n \rightarrow \infty$, we have 
    \begin{align*}
        P(\beta_{ab} \geq k \given \sigf_0) &= P(\nu_{2k} < \infty \given \sigf_0) \\
                &\leq \left( \frac{a}{b} \right)^K \min(1, \frac{Y_0}{a}).  
    \end{align*}
\end{proof}

% section lecture_11_thursday_7_april (end)

\section{Lecture 12 - Thursday 7 April} % (fold)
\label{sec:lecture_12_thursday_7_april}
\begin{thm}
    Let $\{ Y_n \}$ be a positive supermartingale.  Then there exists a random variable $Y_\infty$ such that $Y_n \cas Y_\infty$ and $\E(Y_\infty \given \sigf_n) \leq Y_n$ for all $n$.
\end{thm}
\begin{proof}
    From Durbin's inequality, \[
        P(\beta_{ab} \geq k) \leq \left( \frac{a}{b} \right)^k
    \]  By Borel-Cantelli, as we have a summable sequence of probabilities, $\beta_{ab}  < \infty$ almost surely.  Hence \[
        P(\text{$Y_n$ converges}) = P\left(\bigcap_{\substack{a < b \\ a, b \in \Q}} \beta_{ab} < \infty \right) = 1
    \] Let $\lim_{n \rightarrow \infty}Y_n = Y_\infty$. If $p < n$, then \[
        \E\left( \inf_{m \geq n} Y_n \given \sigf_p \right) \leq \E(Y_n \given \sigf_p) \leq Y_p.   \]  Furthermore, $\inf_{m \geq n} Y_m \uparrow Y_\infty$ so by the monotone convergence theorem, we have \[
            \E(Y_\infty \given \sigf_p) = \lim_{n \rightarrow \infty} \E\left( \inf_{m \geq n} Y_m \given \sigf_p \right) \leq Y_p.
        \]  
\end{proof}

\begin{thm}
    Let $Z$ be a positive random variable with $\E Z^p < \infty$, $p \geq 1$. Then \[
        Y_n = \E(Z_n \given \sigf_n) \cas, \clp \E(Z \given \sigf_\infty), 
    \]  Note that almost sure convergence does not, in general, imply $L^p$ convergence, although they both imply convergence in probability.  
\end{thm}
\begin{proof}
    Suppose $Z \leq a$ almost surely.  Then there exists $Y_\infty$ such that $Y_n \cas Y_\infty$ (as $Y_n$ are positive martingales).  Fix $n$ and let $B \in \sigf_n$.  Then \[
        \lim_{n \rightarrow \infty} \int_B Y_{m+n} \, dP = \int_B Z \, dP
    \] by definition of conditional expectation.  Now $0 \leq Y_n \leq a$ so by the dominated convergence theorem, \[
        \int_B Y_\infty \, dP = \int_B Z \, dP
    \] and hence \[
        Y_\infty = \E(Z \given \sigf_\infty)
    \] and so the random variable $Y_\infty$ can be identified as the conditional expectation.  
    
    Since $|Y_n| \leq a$, the $\{ Y_n^p \}$ are uniformly integrable, and so $Y_n \clp Y_\infty$.  This follows from noting that $Y_n \cas Y_\infty$, and using that if $X_n \cip X$ and $\{ |X_n|^p \}$ is uniformly integrable then $X_n \clp X$.  
    
    Now remove the assumption that $Z \leq a$. Taking the $L^p$ norm of the conditional expectations gives \[
        \| E(Z \given \sigf_n) - \E(Z \given \sigf_\infty) \|_p \leq \| E(Z \wedge a \given \sigf_n) - \E(Z \wedge a \given \sigf_\infty) \|_p + 2 \| (Z - a)^+ \|_p.
    \]  Now we know that $\|(Z -a)^+\|_p \rightarrow 0$ as $a \rightarrow \infty$, as $\E(Z^p) < \infty$.  Hence we have \[
        Y_n \clp \E(Z \given \sigf_\infty).
    \]  By uniqueness of limits, we obtain our required result.  
\end{proof}
\begin{cor}
    If $Z \in L^p$ and $Y_n = \E(Z \given \sigf_n)$ then $Y_n \cas, \clp \E(Z \given \sigf_\infty)$
\end{cor}

\begin{thm}{Martingale convergence theorem}{\ }
    \begin{enumerate}[(a)]
        \item 
        If $\{ Y_n \}$ is an integrable submartingale and $\sup_{n} \E(Y_n^+) < \infty$ then there exists an integrable $Y_\infty$ such that \[
        Y_n \cas Y_\infty
        \]  
        \item
        If $\{ Y_n \}$ is an integrable martingale satisfying $\sup_n \E |Y_n| < \infty$ then there exists an integrable $Y_\infty$ such that \[
            Y_n \cas Y_\infty.
        \]
    \end{enumerate}
\end{thm}

\begin{proof}{\ }
    \begin{enumerate}[(a)]
        \item $\{ Y_n^+ \}$ is a positive submartingale as \[
            \E(Y_{n+1}^+ \given \sigf_n) \geq \E(Y_{n+1} \given \sigf_n) \geq Y_n
        \]  If $p > n$, then \begin{align*}
            \E(Y_{p+1}^+ \given \sigf_n) &= \E(\E(Y_{p+1}^+ \given \sigf_p) \given \sigf_n) \\
            &\geq \E(Y_{p^+} \given \sigf_n). 
        \end{align*}   Hence $M_n = \lim_{p \rightarrow \infty}\E(Y_p^+ \given \sigf_n)$ as we have a monotone sequence.   
        
        Now, \begin{align*}
            \E(M_n) &= \E\left(\lim_{p \rightarrow \infty} \E(Y_p^+ \given \sigf_n) \right) \\
            &= \lim_{p \rightarrow \infty} \E(\E( Y_p^+ \given \sigf_n)) \quad \text{MCT} \\
            &= \lim_{p \rightarrow \infty} \E(Y_p^+) < \infty
        \end{align*} so $M_n$ is positive and integrable.  $M_n$ is a martingale as \begin{align*}
            \E(M_{n+1} \given \sigf_n) &= \E\left(\lim_{p \rightarrow \infty} \E(Y_p^+ \given \sigf_{n+1}) \given \sigf_n \right)  \\
            &= \lim_{p \rightarrow \infty} \E(Y_p^+ \given \sigf_n) \quad \text{MCT} \\
            &= M_n.
        \end{align*}  
        
        Let $Z_n = M_n - Y_n$. Then $Z_n$ is integrable as $M_, Y_n$ are, and $Z_n$ is a positive supermartingale, as \begin{align*}
            \E(Z_{n+1} \given \sigf_n) &= \E(M_{n+1} \given \sigf_n) - \E(Y_{n+1} \given \sigf_n) \\
            &\leq M_n - Y_n \quad \text{as $Y_n$ is submartingale} \\
            &= Z_n
        \end{align*} and so $Z_n$ is a positive supermartingale.  Note that $M_n \geq Y_{n}^+$ and so \[
            M_n - Y_n = M_n -(Y_n^+ - Y_n^-) \geq Y_n^+ - (Y_n^+ - Y_n^-) = Y_n^-
        \]  Thus $Z_n$ and $M_n$ converge almost surely to $Z_\infty$ and $M_\infty$ respectively, and so \[
            Y_n = M_n - Z_n \cas M_\infty - Z_\infty = Y_\infty \in L^1.  
        \]
        \item Note that $|Y_n| = 2Y_n^+ - Y_n$, and if $\{ Y_n \}$ is a martingale, then \begin{align*}
            \E |Y_n| &= 2 \E Y_n^+ - \E Y_n \\
            & 2 \E Y_n^+ - \E Y_0 
        \end{align*} and so $\sup \E Y_n^+ < \infty$ if and only if $\sup_n \E |Y_n| < \infty$.  
    \end{enumerate}
\end{proof}
\begin{thm}[Martingale convergence theorem (restated)]
    Let $\{Y_n\}$ be an integrable (sub/super) martingale, that is, $\sup_n \E |Y_n| < \infty$.  Then there exists an almost sure limit \[
        \lim_{n \rightarrow \infty} Y_t = Y_\infty
    \] and $Y_\infty$ is an integrable random variable.
\end{thm}
% section lecture_12_thursday_7_april (end)

\section{Lecture 13, 14 - Thursday 14 April} % (fold)
\label{sec:lecture_13_thursday_14_april}

\begin{defn}[Reverse martingale]
    $\{ Y_n , \sigg_n \}$ is a reverse martingale if $ \{ \sigg_n \}$ is a decreasing sequence of $\sigma$-fields, \[
        \sigg_n \supseteq \sigg_{n+1}
    \] $Y_n$ is $\sigg_n$-measurable, $\E(|Y_n|) < \infty$, and \[
        \E(Y_n \given \sigg_n) = Y_m \, \text{a.s for $m \geq n$}
    \]
\end{defn}

\begin{prop}
    We have \begin{align*}
        \E(|Y_n|) &= \E( \E(|Y_n| \given \sigg_{n+1})) \\
                    &\geq \E(| \E(Y_n \given \sigg_{n+1})|) \\
                    &= \E(|Y_{n+1}|)
    \end{align*}
    and so $\E(|Y_n|) \leq \E(|Y_0|)$ for all $n$, and \[
        Y_n = \E(Y_0 \given \sigg_n).
    \]
\end{prop}

\begin{thm}
    If $\{ Y_n \}$ is a reverse martingale with respect to $\{\sigg_n \}$, then there exists a random variable $Y_\infty$ such that \[
        Y_n \cas Y_\infty, Y_n \clone Y_\infty = \E(Y_0 \given \sigg_\infty)
    \] where $\sigg_\infty = \bigcap {\sigg_n}$.
\end{thm}

\begin{proof}
    We have $Y_n = \E(Y_0 \given \sigg_n)$ and so $\{ Y_n \}$ is uniformly integrable.  Hence if $Y_n \cas Y_\infty$ it also converges in $L^1$.  Let \[
        Z_n = \E(Y_0^+ \given \sigg_n) - Y_n.
    \]  Note that $Z_n \geq 0$.  Then \[
        \E(Z_n \given \sigg_{n+1}) = Z_{n+1}
    \] and so we only need to consider convergence for positive reverse martingales.  
    
    Let $\beta_{a,b}^{(n)}$ be the number of upcrossings of $[a,b]$ by $\{ Y_0, Y_1, \dots, Y_n \}$.  Applying Dubin's inequality to the martingale \[
        \{ Y_n, Y_{n+1}, \dots, Y_{1}, Y_0 \}
    \] Then \[
        P(\beta_{a,b}^{(n)} \geq k \given \sigg_n) \leq \left( \frac{a}{b} \right)^k
    \] which is independent of $n$, and thus \[
        P(\beta_{a,b}^{(n)} \geq k \given \sigg_\infty) \leq \left( \frac{a}{b} \right)^k
    \] for all $n$, and so \[
        P(\beta_{a,b} \geq k \given \sigg_\infty) \leq \left( \frac{a}{b} \right)^k.
    \] where $\beta_{a,b}$ is the number of upcrossings for $\{ Y_n \}$, which implies \[
        \beta_{a,b} < \infty \, \text{a.s.}
    \] 
    
    Arguing as in the positive supermartingale case, we have $\{ Y_n \}$ converges almost surely, and we have $Y_\infty = \limsup Y_n$ is $\sigg_n$ measurable for all $n$ and so is $\sigg_\infty$ measurable.  
\end{proof}

\begin{thm}[Strong law of large numbers]
    Let $X_1, X_2, \dots$ be IID with $\E(|X_1|) < \infty$.  Let $\E(X_1) = \mu$.  Let $S_n = \sumni X_i$.  Then \[
        \frac{1}{n} S_n \cas \mu.
    \]
\end{thm}

\begin{proof}
    Let $\sigg_n = \sigma \{ S_n , S_{n+1}, S_{n+2}, \dots \} = \sigma \{ S_n, X_{n+1}, X_{n+2}, \dots \}$.  We then have $\sigg_n \supseteq \sigg_{n+1}$. We have \begin{align*}
        \frac{1}{n} S_n &= \E(\frac{1}{n} S_n \given \sigg_n) \\
            &= \frac{1}{n} \sumni \E(X_i \given \sigg_n) \\
            &= \E(X_1 \given \sigg_n),
    \end{align*}  as \[
        \E(X_1 \given \sigg_n) = \E(X_2 \given \sigg_n) = \dots \E(X_n \given \sigg_n)
    \] by IID/symmetry.
    
    Thus $\frac{1}{n} S_n$ is a reverse martingale with respect to $\{ G_n \}$.  From above, we have have \[
        \frac{1}{n} S_n = \overline X_n \cas, \clone \E(X \given \sigg_\infty). 
    \]
    
    We have $\lim_{ n \rightarrow \infty} \sumni X_i$ is in the tail $\sigma$-field of the sequence of $\{ X_n \}$ and $X_i$ are IID and so the limiting random variable is degenerate.  
    
    Consider $\overline X_\infty = \E(X \given \sigg_\infty)$.  By the Kolmogorov 0-1 law, we have \[
        P(\{ \overline X_\infty \leq a \}) = \text{0 or 1}.   
    \]
    
    Thus $\overline X_\infty$ is a constant with probability one.  Since \begin{align*}
        \E(X_1 \given \sigg_n) \clone \E(X_1 \given \sigg_\infty)
    \end{align*}  we have \[
        \lim_{n \rightarrow \infty} \E(\frac{1}{n} S_n) = \E( \E(X_1 \given \sigg_\infty)) = \E(X_1) = \mu.
    \]
    
    Thus $\overline X_\infty = \mu$ almost surely, that is, \[
        \frac{1}{n} S_n \cas, \clone \mu.
    \]
\end{proof}

\subsection{Characteristic functions} % (fold)
\label{sub:characteristic_functions}

Following \textbf{Fallow Volume 2}.
\begin{defn}[Characteristic function]
    Let $X$ be a random variable. Then the characteristic function is defined by \[\phi(t) = \E(e^{itX}).\]  
    
    $\phi(t)$ is always defined (unlike moment generating function (MGF), probability generating function (PGF)). 

\end{defn}

\begin{proof}
    Let $\phi(t)$ be the characteristic function of the random variable $X$.  Then 
    \begin{enumerate}[(i)]
        \item  $| \phi(t) | \leq \E(|e^{itX}|) = 1 = \phi(0).$ 
        \item $\phi(-t) = \E(e^{-itX}) = \overline{ \phi(t)}.$
        \item If $X$ is symmetric about 0 then $\phi(t)$ is real. 
        \item $\phi(t)$ is uniformly continuous in $t$.  \begin{proof}
            \begin{align*}
                |\phi(t+h) - \phi(t) &= \left| \int e^{i(t+h)X} - e^{itX} \, dF(x) \right| \\
                            &= \left| \int e^{itX}(e^{ihX} - 1) \, dF(x) \right| \\ 
                            &\leq \int \left| e^{ihX} - 1 \right|\, dF(x) \\
                            & = \int \sqrt{\cos^2(xh - 1) + \sin^2(xh)} \, dF(x) \\
                            &= \int\sqrt{2 - 2\cos hx} \, dF(x) \rightarrow 0
            \end{align*} as $h \rightarrow 0$ by the dominated convergence theorem.
        \end{proof}
        \item If $X$ and $Y$ are independent random variables with characteristic functions $\phi$ and $\psi$ respectively, then $X + Y$ has characteristic function \[
            \chi(t) = \phi(t) \cdot \psi(t)
        \]
        \item If $X$ has a characteristic function $\phi$ then $aX + b$ has a characteristic function $e^{itb} \phi(at)$.
        \item If $\phi$ is a characteristic function the so is $| \phi |^2$.  
        \begin{proof}
            Let $X$ and $Y$ have the same distribution, with $X$ independent of $Y$.  Then $Z = X-Y$ has a characteristic function $\phi(t) \phi(-t) = |\phi(t)|^2$.
        \end{proof}
        \item Let $X$ have a MGF $M(t)$.  Then $\phi(t) = M(it)$.
    \end{enumerate}
\end{proof}

\begin{exmp}
    \begin{enumerate}[(i)]
        \item Let $X \sim N(0,1)$.  Then \[\phi(t) = e^{-\frac{1}{2}t^2}.
        \]  
        \item Let $Y \sim N(\mu, \sigma^2)$.  Then \[
            \phi(t) = e^{it\mu - \frac{1}{2} \sigma^2 t^2}.
        \] as $Y = \mu + \sigma Z$ with $Z \sim N(0,1)$.
        \item Let $X \sim \text{Poisson}(\lambda)$  Then \[
            \phi(t) = e^{\lambda(e^{it} - 1)}.
        \]
        \item Let $P(X= 1) = \frac{1}{2} = P(X = -1)$.  Then \[
            \phi(t) = \frac{1}{2}\left(e^{it} + e^{-it} \right) = \cos t. 
        \]
        \item Let $X \sim \text{Exp}(\lambda)$.  Then \begin{align*}
            \phi(t) &= \int_0^\infty e^{itx} \lambda e^{-\lambda x} \, dx \\
                    &= \int_0^\infty \lambda e^{-x(\lambda - it)} \, dx \\
                    &= \frac{\lambda}{\lambda - it} 
        \end{align*}
    \end{enumerate}
\end{exmp}

\begin{thm}[Parseval's relation]
    Let $F$ and $G$ be distribution functions with associate characteristic functions $\phi$ and $\psi$.  Then \[
        \int e^{-izt} \phi(z) \, dG(z) = \int \psi(x - t) \, dF(x)
    \]
\end{thm}
\begin{proof}
    \begin{align*}
    \int e^{-izt} \phi(z) \, dG(z)
            &= \int e^{-izt} \left(\int e^{izt} \, dF(x) \right) \, dG(z) \\
            &= \int \int e^{iz(x-t)} \, dF(x) \, dG(x) \\
            &= \int \left( \int e^{iz(x-t)} \, dG(z) \right) \, dF(x) \quad \text{by Fubini's theorem}\\
            &= \int \psi(x-t) dF(x)
    \end{align*}
\end{proof}

\begin{cor}
    If $G$ is the distribution function of a $N(0, \frac{1}{\sigma^2})$ random variable.  Then $\psi(t) = e^{-\frac{1}{2\sigma^2} t^2}$, and so the above relationship becomes \[
        \int e^{izt} \phi(z) \frac{\sigma}{\sqrt{2\pi}} e^{-\frac{1}{2}z^2 \sigma^2} \, dz = \int e^{-\frac{1}{2\sigma^2}(x-t)^2} \, dF(x).
    \]  Rearranging, we obtain \[
        \frac{1}{2\pi} \int e^{-izt} \phi(z) e^{-\frac{1}{2} z^2 \sigma^2} \, dz = \int \frac{1}{\sqrt{2 \pi \sigma^2}} e^{- \frac{1}{2 \sigma^2}(x - t)^2} \, dF(x)
    \]  Then the right hand side is the density of the convolution of $F$ and a $N(t, \sigma^2)$ distribution.  Call the convolution distribution $F_\sigma$.  Then \begin{align*}
        F_\sigma(\beta) - F_\sigma(\alpha) &= \int_\alpha^\beta \left(  \frac{1}{2\pi} \int e^{-izt} \phi(z) e^{-\frac{1}{2} z^2 \sigma^2} \, dz \right) \, dt \\
        &= \frac{1}{2\pi} \int \phi(z) e^{-\frac{1}{2} z^2 \sigma^2} \frac{e^{-iz\beta} - e^{-iz\alpha}}{-iz} \, dz
    \end{align*}
    
    If $\alpha$ and $\beta$ are continuity points of $F$, then \[
        F(\beta) - F(\alpha) = \lim_{\sigma \rightarrow 0} \frac{1}{2 \pi} \int \phi(z) e^{-\frac{1}{2} z^2 \sigma^2} \frac{e^{-iz\beta} - e^{-iz\alpha}}{-iz} \, dz \tag{$\star$}
    \] for as $\sigma \rightarrow 0$, $F_\sigma \rightarrow F$.  
    
    Since a function has only countably many points of discontinuity, we can then derive the following theorem.
\end{cor}

\begin{thm}
    Let $X$ be a random variable with distribution function $F$ and characteristic function $\phi$.  Assume \[
        \int | \phi(t) | \, dt < \infty.
    \]  Then $F$ has a bounded, continuous density $f$ given by \[
        f(x) = \frac{1}{2\pi} \int e^{-itx} \phi(t) \, dt
    \]
\end{thm}

\begin{proof}
    From $(\star)$ apply DCT.  Then \begin{align*}
        F(\beta) - F(\alpha) = F(\beta) - F(\alpha) &= \lim_{\sigma \rightarrow 0} \int_\alpha^\beta \left( \frac{1}{2 \pi} \int \phi(z) e^{-\frac{1}{2} z^2 \sigma^2} \, dz \right) \, dt \\
        &= \int_\alpha^\beta \left( \frac{1}{2 \pi} \int e^{-izt} \phi(z) \, dz \right) \, dt 
    \end{align*}
\end{proof}

\begin{cor}\label{cor:duality}
    If $\phi(t)$ is non-negative and integrable continuous function associated with a distribution function $F$.  Then $\frac{\phi(t)}{2 \pi F'(0)}$ is a density function with characteristic function $\frac{F'(x)}{F'(0)}$.
\end{cor}

\begin{proof}
    We have \begin{align*}
        F'(x)   &= \frac{1}{2\pi} \int e^{-izx} \phi(z) \, dz \\
                &= \frac{1}{\pi} \int_0^\infty \cos(xz) \phi(z) \, dz \quad \text{as $\phi(z)$ is real}         \end{align*}
    Thus \begin{align*}
        F'(0)   &= \frac{1}{\pi} \int_0^\infty \phi(z) \, dz  \\
        1       &= \frac{1}{2 F'(0) \pi} \int \phi(z) \, dz 
    \end{align*}
    and thus 
    \begin{align*}
        \frac{F'(x)}{F'(0)} &= \int \cos(xz) \frac{\phi(z)}{2 \phi F'(0)} \, dz
    \end{align*}
\end{proof}
% subsection characteristic_functions (end)
% section lecture_13_thursday_14_april (end)
\section{Lecture 14 - Thursday 14 April} % (fold)
\label{sec:lecture_14_thursday_14_april}

% section lecture_14_thursday_14_april (end)

\section{Lecture 15 - Thursday 21 April} % (fold)
\label{sec:lecture_15_thursday_21_april}
\begin{exmp}
    $X$ has density $f(x) = \frac{1}{2} e^{-|x|}$.  Then \begin{align*}
        \phi(t) &= \frac{1}{2} \int e^{itx} e^{-|x|} \, dx \\
        &= \int_0^\infty \cos tx e^{-x} \, dx \\
        &= \int_0^\infty \frac{1}{2} \left( e^{itx} + e^{-itx} \right) e^{-x} \, dx \\
        &= \frac{1}{2} \int_0^\infty e^{-x(1-it)} + ^{-x(1 +it)} \, dx \\
        &= \frac{1}{2} \left[ \frac{-1}{1-it}e^{-x(1+it)} + \frac{-1}{1+it}e^{-x(1+it)} \right]_0^\infty \\
        &= \frac{1}{1+t^2}
    \end{align*}  Thus $\phi(t) = \frac{1}{1+t^2}$ which is a non-negative, integrable characteristic function.  Thus, \[
        \frac{\phi(t)}{2\pi f(0)} = \frac{1}{\pi(1+t^2)} 
    \] which is the Cauchy distribution.  We then know that the characteristic function of the Cauchy distribution is \[
        \gamma(t) = \frac{F'(x)}{F'(0)} = \frac{f(x)}{f(0)} = e^{-|t|}
    \] from Corollary \ref{cor:duality}.
\end{exmp}

\begin{thm}[Moment theorem]
    Let $F$ be the distribution function of $X$.  Assume $X$ has finite moments up to order $n$, i.e. $\E(|X|^n) < \infty$.  Then the characteristic function $\phi(t)$ has uniformly continuous derivatives up to order $n$, and \[
        \phi^{(k)}(t) = i^k \E(|X|^k), k = 1, 2, \dots, n
    \] and \[
        \phi(t) = 1 + \sum_{k=1}^n \E(X^k) \frac{(it)^k}{k!} + o(t^n)
    \] as $t \rightarrow 0$. 
    
    Conversely, if $\phi$ can be written as \[
    \phi(t) = 1 + \sum_{k=1}^n a_k \frac{(it)^k}{k!} + o(t^n)
    \] as $t \rightarrow 0$, then the associated density function has finite moments up to order $n$ if $n$ is even, and up to order $n-1$ if $n$ is odd, with $a_k = \E(|X|^k)$.   
\end{thm}
\begin{proof}

\begin{lem}\label{lem:taylor}
    For any $t \in \R$, \[
        \left|  e^{it} - 1 - it \dots - \frac{(it)^{n-1}}{(n-1)!} \right| \leq \frac{|t|^n}{n!}.
    \]
\end{lem}
\begin{proof}
    Taylor's Theorem.
\end{proof}

Suppose $\E(|X|^k) < \infty$ for $k = 1, 2, \dots, n$.  Then \[
    |x^k e^{itx} | \leq |x|^k\], so \[
        \int x^k e^{itx} \, dF(x)
    \]  exists.  Now \begin{align*}
        \frac{\phi(t+h) - \phi(t)}{h} &= \left| \int \frac{e^{i(t+h)x} - e^{itx}}{h} \, dF(x) \right| \\
        &= \left| \int e^{itx} \cdot \frac{e^{ihx} - 1}{h} \, dF(x) \right| \\
        &\leq \int |x| \, dF(x) < \infty
    \end{align*} from Lemma \ref{lem:taylor}.
    
    So by DCT, \begin{align*}
        \phi'(t) = \lim_{h \rightarrow 0} \frac{\phi(t+h) - \phi(t)}{h} &= i \int x e^{itx}  \, dF(x)
    \end{align*} and thus \[
        \phi'(0) = i \E(X).
    \]

    Using induction, we obtain \[
        \phi^{(k)}(t) = i^k \int x^k e^{itx} \, dF(x)
    \] and $\phi^{(k)}(0) = i^k \E(X^k)$ for $k = 1, 2, \dots, n$. 
    
    Arguing as in the proof of characteristic functions uniform continuity.  
    
    Expanding $\phi(t)$ about $t = 0$ in a Taylor series, we have \[
        \phi(t) = 1 + \sumnk \phi^{(k)}(0) \frac[t^k]{k!} + R_n(t), t > 0.
    \] with \[
        R_n(t) = \frac{t^n}{n!} \left[ \phi^{(n)}(\theta t) - \phi^{(n)}(0) \right], 0 < \theta < 1.
    \]  
    
    We then have 
    \begin{align*}
        \left| \frac{R_n(t)}{t^n} \right| &\leq \frac{1}{n!} \int |x|^n | e^{i\theta tx} - 1 | \, dF(x) \\
        &\leq \frac{2}{n!} \int |x|^n \, dF(x).
    \end{align*} and so by the DCT, \[
        \lim_{t \rightarrow 0} \left| \frac{R_n(t)}{t^n} \right| = 0, 
    \] and thus $R_n(t) = o(t^n)$.
    
    Conversely, suppose $\phi$ has an expansion up to order $2k$.  Then $\phi$ has a finite derivative of order $2k$ at $t = 0$  Then \begin{align*}
        -\phi^{(2)}(0) = -\lim_{h \rightarrow 0} \frac{ \phi(h) - 2 \phi(0) - \phi(-h)}{h^2} \\
        &= \lim_{h \rightarrow 0} 2 \int \frac{1 - \cos hx}{x^2} \, dF(x) \\
        &\geq 2 \int \lim_{h \rightarrow 0} \frac{1 - \cos hx}{h^2} \, dF(x) \, \text{by Fatau} \\
        &= \int x^2 dF(x) = E(X^2)
    \end{align*} and so $\phi^{(2)}(0) < \infty \Rightarrow \E(X^2) < \infty$.    
    
    Using induction, assume finite $2(k-1)$\textsuperscript{th} derivative at $0 \Rightarrow \E(X^{2(k-1)}) < \infty$.  Then from the first part, \[
        \phi^{(2(k-2))}(t) = (-1)^{k-1} \int x^{2k-2} e^{itx} \, dF(x) 
    \]  Suppose $\phi^{2k}(0) < \infty$.  Then let \[
        G(x) = \int_{-\infty}^x y^{2k-2} \, dF(y).  
    \] so $\frac{G(x)}{G(\infty)}$ is a distribution function with characteristic function \begin{align*}
    \psi(t) &= \frac{1}{G(\infty)} \int e^{itx} x^{2k-2} \, dF(x) \\
            &= \frac{(-1)^{k-1} \phi^{(2k-2)}(t)}{G(\infty)}
    \end{align*}
    As $\phi^{(2k-2)}(t)$ is twice differentiable at $t = 0$.  So \[
        \psi^{(2)}(0) \geq \int y^2 y^{2k-2} \, \frac{dF(y)}{G(\infty)} 
    \] and thus $\E(X^{2k}) < \infty$.  as required.
\end{proof}
% section lecture_15_thursday_21_april (end)

\section{Lecture 16 Thursday 21 April} % (fold)
\label{sec:lecture_16_thursday_21_april}
    \begin{cor}
        Let $\phi$ be a characteristic function associated with a random variable $X$.  Then $\phi$ has continuous derivatives of all orders if and only if $X$ has finite moments of all orders.  
    \end{cor}
    
    \begin{cor}
        The function $\phi(t) = e^{-|t|^\alpha}$ is not a characteristic function if $\alpha > 2$.  Note that $\alpha = 1$ was the Cauchy distribution, $\alpha  = 2$ is the Normal distribution.
    \end{cor}
    
    \begin{proof}
        If $\alpha > 2$ then \[
            \lim_{t \rightarrow 0 } \phi^{(2)}(t) = 0 \Rightarrow \E(X^2) = 0
        \] which implies $X$ is degenerate.  But if $X$ is degenerate at $b$, then \[
            \phi(t) = e^{itb} \neq e^{-|t|^\alpha}
        \]  Thus by uniqueness of characteristic functions, $e^{-|t|^\alpha}$ is not a characteristic function.
    \end{proof}

\subsection{Lattice distributions} % (fold)
\label{sub:lattice_distributions}

% subsection lattice_distributions (end)
\begin{thm}[Lattice distributions]
    Let $X$ be a random variable with distribution function $F$, characteristic function $\phi$.  If $c \neq 0$ then the following are equivalent.
    \begin{enumerate}[(i)]
        \item $X$ has a lattice distribution whose range is continued in $0, \pm b, \pm 2b, \dots$, $b = \frac{2 \pi}{c}$.
        \item $\phi(t + nc) = \phi(t)$ for $n = \pm 1, \pm 2, \dots$, that is, $\phi$ is periodic with period $c$. 
        \item $\phi(c) = 1$.  
    \end{enumerate}
\end{thm}
\begin{proof}
    $(1) \Rightarrow (2)$.
    \begin{align*}
        \phi(t) &= \sum_{k=-\infty}^\infty P(X = kb) e^{itkb} \\
        &= \sum_{k=-\infty}^\infty P(X=kb) e^{2\pi i t k /c} \\
    \end{align*} which implies \[
        \phi(t + nc) = \phi(t)
    \] as $e^{2 \pi i n c k /c} = 1$.  
    
    $(2) \Rightarrow (3)$.  Simply set $t = 0, n = 1$.  Then $\phi(0) = \phi(c) = 1$.  
    
    $(3) \Rightarrow (1)$.  \begin{align*}
        1 - \E(\cos cX) &= 0 \\
        \E(1 - \cos cX) &= 0 
    \end{align*}
    but as $1 - \cos cX \geq 0$, $X$ must have probability components on points where $\cos cX = 1$, that is, $c X$ takes on the values $0, \pm \pi, \pm 2 \pi, \dots$.  
\end{proof} 

\begin{cor}
    $X$ is degenerate if and only if $| \phi(t) | = 1$ for all $t$. 
\end{cor}

\begin{proof}
    If $P(X = b) = 1$, then $\phi(t) = e^{itb},$ and so $| \phi(t) | = 1$ for all $t$.
    
    If $|\phi(c)| = 1$ for $ c \neq 0$, then $\phi(c) = e^{i\theta}$ for some $\theta$.  Let $\phi_1(t) = \phi(t) e^{-i \theta t/c}$ is characteristic function of $X - \frac{\theta}{c}$.  Then $\phi_1(c) = 1$, thus $X- \frac{\theta}{c}$ is a lattice taking values in $0, \pm \frac{2\pi}{c}, \pm \frac{4\pi}{c}, \dots$.  
    
    Now, pick some $b \in \R$ with $\frac{b}{c}$ irrational.  Then $|\phi(b) | = 1$, and then $X - a_2$ is a lattice taking values in $0, \pm \frac{2 \pi}{b}, \pm \frac{4 \pi}{b}, \dots$.  Then \begin{enumerate}[(i)]
        \item $|\phi(t)| < 1$ for $t \neq 0$ (e.g. Normal, $e^{-\frac{1}{2}t^2}$).
        \item $|\phi(\lambda) | = 1$ and $|\phi(t)| < 1$ on $0 < t < \lambda$ (e.g. discrete $\pm 1$, $\cos t$).
        \item $|\phi(t)| = 1 \forall t$, degenerate distributions.  
    \end{enumerate}
\end{proof}  


\begin{exmp}
    We can construct 3 nontrivial distribution functions $\phi_1, \phi_2, \phi_3$ such that 
\begin{enumerate}[(i)]
    \item $\phi_1(t) = \phi_2(t), \forall t \in [-1,1]$.  
    \item $|\phi(t)| = | \phi_3(t) |, \forall t$.  
\end{enumerate}

Consider $g(x) = 1 - | x |, x \in [ -1, 1]$.  This has characteristic function $\phi(t) = \frac{2(1 - \cos t)}{t^2}$.  But the characteristic function is positive and integrable, and so\[
    \phi_1(t) = \begin{cases}
        1 - |t| & |t| \leq 1 \\
        0       & |t| > 1
    \end{cases}
\] is the characteristic function of the density \[
    f(x) = \frac{1 - \cos x}{\pi x^2}.
\]  We can express $\phi_1(t)$ as the trigonometric series, \begin{align*}
    \phi_1(t) = 1 - |t| = \frac{1}{2} + \sum_{k=1}^\infty a_k \cos( k \pi t) 
\end{align*} with \[
    a_k = 2 \int_0^1 ( 1 - t) \cos(k \pi t) \, dt = \begin{cases} 
        \frac{4}{k \pi^2}   &\text{$k$ odd} \\
        0                   &\text{$k$ even}
    \end{cases}
\]
We can thus write \[
    \phi_1(t) = \frac[1]{2} + \sum_{k=1}^\infty \frac{4}{(2k-1)^2 \pi^2} \cos(2k-1) \pi t.
\]

Let $V$ be a random variable, with \[
    P(V = 0) = \frac{1}{2}, P(V = \nu) = \frac{2}{\nu^2}, \nu = \pm \pi, \pm 3 \pi, \pm 5 \pi, \dots
\]    Then $V$ is a lattice distribution, with characteristic function \[
    \phi_2(t) = \frac{1}{2} + \frac{4}{\pi^2} \left( \cos \pi t + \frac{ \cos 3 \pi t}{9} + \frac{\cos 5 \pi t}{25} + \dots \right) 
\] and thus $\phi_1(t) = \phi_2(t)$ on $[-1,1]$, but have different density functions.

Finally, let $U$ be a lattice random variable with distribution \[
    P(U = \pm \frac{(2k+1) \pi}{2}) = \frac{4}{\pi^2(2k+1)^2}, k = 0,1, 2, \dots
\]  Then $U$ has a characteristic function $\phi_3(t) = 2 \left[ \phi_2(\frac{t}{2}) - \frac{1}{2} \right]$.  Thus \[
    | \phi_3(t) | = | \phi_2(t) | \quad \forall t.
    \]
\end{exmp}
% section lecture_16_thursday_21_april (end)

\section{Lecture 17 - Thursday 5 May} % (fold)
\label{sec:lecture_17_thursday_5_may}
\subsection{Sequences of characteristic functions} % (fold)
\label{sub:sequences_of_characteristic_functions}

% subsection sequences_of_characteristic_functions (end)
\begin{lem}[Helly selection theorem]\label{lem:hellysel}
    Given a sequence of distribution functions $\{ F_n \}$ then there exists a sequence $\{ n_k \}$ and a non decreasing right continuous function $F$ such that \[
        F_{n_k}(x) \rightarrow F(x)
    \] at all continuity points $x$ of $F$.  
\end{lem}
\begin{proof}
    First order the rationals to get a sequence $\{ r_k \}$.  From $\{ F_n(r_1)\}$ we choose a subsequence $\{ F_{n_{1k}}(r_1) \}$ which converges.  
    
    Now from the sequence $\{ n_{1k} \}$ choose a subsequence $\{ n_{2k} \}$ such that $\{ F_{n_{2k}}(r_2) \}$ converges, etc.
    
    Now let $n_k = n_{kk}$.  Then for each rational number $r$, the limit $F_{n_k}(r)$ exists as $n \rightarrow \infty$.  Define $L(R) = \lim F_{n_k}(r), r \in \Q$.  Then $L(r)$ is non-decreasing and takes values in $[0,1]$.  Let $F(x) = \inf_{r \leq x} L(r)$.  Then $F$ is non-decreasing, and right continuous, and $F_{n_k}(x) \rightarrow F(x)$ for all $x \in \Q$ and at all points of continuity of $F$.  
\end{proof}

\begin{lem}[Extended Helly-Bragg theorem] \label{lem:extendedhelly}
    If a sequence of distribution functions $\{ F_n \}$ converges to a function $F$ at all continuity points of $F$ and $g$ is a \textbf{bounded, continuous, real valued function} then \[
        \int_\R g \,dF_n \rightarrow \int_\R g \, dF
    \]
\end{lem}
\begin{proof}
    Let $M = \sup_x | g(x) |$, and let $a,b$ be continuity points of $F$.  Then 
    \begin{align*}
        \left| \int_\R g \, dF_n - \int_\R g \, dF \right| &\leq \left| \int_\R g \, dF_n - \int_a^b g \, dF_n \right| + \left| \int_a^b g \, dF_n - \int_a^b g \, dF \right| + \left| \int_a^b g \, dF - \int_\R g \, dF \right| \\
        &\leq M[F_n(a) - F_n(-\infty) + F_n(\infty) - F_n(b)] + \left| \int_a^b g \, dF_n - \int_a^b g \, dF \right| \\
        &\qquad  + M[F(a) - F(-\infty) + F(\infty) - F(b)] 
    \end{align*} Since \[
        F_n(a) \rightarrow F(a), F_n(b) \rightarrow F(b)
    \] as $a, b$ are continuity points, we can choose $a, b$ large enough to make the 3rd term small $(< \frac{\epsilon}{3}$ for arbitrary $\epsilon > 0$), and then $N$ large enough to make the first term small. 
    
    Now we deal with the middle term.  Let $a = x_{0N} < x_{1N} < \dots < x_{\nu_N, N} = b$ be a sequence of subdivisions of $[a,b]$, such that $\Delta_{n} \rightarrow 0$ (partition width) as $n \rightarrow \infty$.  Then \[
        g_N(x) = \sum_{\nu = 1}^{\nu_N} g(x_{\nu}, N) \indic{x_{\nu-1, N} \leq x \leq x_{\nu, N}} 
    \]  Then $\sup_{x \in [a,b]} | g_N(x) - g(x)| \rightarrow 0$ as $N \rightarrow \infty$ (as $g$ is bounded and continuous.) Then by DCT we have 
    \begin{align*}
        \int_a^b g \, dF_n &= \lim_{N \rightarrow \infty} \int_a^b g_N \, dF_n \\
        \int_a^b g \, dF &= \lim_{N \rightarrow \infty} \int_a^b g_N \, dF
    \end{align*}   Next, we will show \[
        \lim_{ n \rightarrow \infty} \int_a^b g_N \, dF_n = \int_a^b g_N \, dF
    \]  Let $x_{\nu, N}$ be continuity points of $F$ so \[
        F_n(x_{\nu, N}) - F_n(x_{\nu - 1, N}) \rightarrow F(x_{\nu, N}) - F(x_{\nu - 1, N}).
    \] Hence
    \begin{align*}
        \lim_{n \rightarrow \infty} \int_a^B g_N(x) \, dF_n &= \lim_{n \rightarrow \infty} \sum_{\nu = 1}^{\nu_N} g(x_{\nu, N}) (F_n(x_{\nu, N}) - F_n(x_{\nu - 1, N})) \\
        &= \int_a^b g_N(x) \, dF(x) 
    \end{align*} If $M_N = \sup_{x \in [a,b]} | g_N(x) - g(x) |$, then \begin{align*}
        \left| \int_a^b g \, dF_N - \int_a^b g \, dF \right| &\leq \int_a^b |g - g_n | \, dF_n + \left| \int_a^b g_n \, dF_n - \int_a^b g_N \, dF \right| + \int_a^b |g - g_N | \, dF \\
        &\leq M_N [ F_n(b) - F_n(a)] + \left| \int_a^b g_N \, dF_n  \right.\\
        &\qquad \left. \int_a^b g_N \, dF \right| + M_N[F(b) - F(a)] 
    \end{align*} Since $M_N \rightarrow 0$ as $N \rightarrow \infty$.  Then choosing $N$ large enough to make $M_N$ small enough, for a large $N$ fixed, $N_2$ say, we have \[
        \left| \int_a^b g_{N_2} \, dF_n - \int_a^b g_{N_2} \, dF \right| \leq \frac{\epsilon}{9}
    \]  The result then follows.
\end{proof}  

\begin{lem}
    Let $\{ F_n \}$ be a sequence of distribution functions with associated characteristic function $\{ \phi_n \}$.  Assume $\phi_n(t) \rightarrow \phi(t)$ as $n \rightarrow \infty$ for all $t \in \R$.  Then there exists a non-decreasing right continuous function $F$ such that $F_n(x) \rightarrow F(x)$ at all continuity points $x$ of $F$. 
\end{lem}
\begin{proof}
    From Lemma \ref{lem:hellysel} there exists a subsequence $\{ n_k \}$ and a non-decreasing continuous function $F$ such that $F_{n_k} (x) \rightarrow F(x)$ at all continuity points of $F$.  Using Parseval's relation on $\{ F_{n_k}, \phi_{n_k} \}$, we have \begin{align*}
        \frac{1}{2 \pi} \int_\R e^{-izt} \phi_{n_k}(z) e^{- \frac{1}{2} \sigma^2 z^2} \, dz = \int_\R \frac{1}{\sqrt{2 \pi}} e^{- \frac{1}{2} \left( \frac{x-t}{\sigma} \right)^2} \, dF_{n_k}(x)
    \end{align*}  Let $k \rightarrow \infty$.  Then the LHS becomes \[
        \frac{1}{2 \pi} \int_\R e^{-i z t} \phi(z) e^{- \frac{1}{2} \sigma^2 z^2} \, dz 
    \] by the dominated convergence theorem. 
    
    The RHS becomes \[
        \int_\R \frac{1}{\sqrt{2 \pi}} e^{- \frac{1}{2} \left( \frac{x-t}{\sigma} \right)^2} \, dF(x)
    \] by an application of Lemma \ref{lem:extendedhelly}.  Thus $\phi$ determines $F$ uniquely (as before), so the limit $F$ must be the same for all convergent subsequences.  
\end{proof}

\begin{thm}[Continuity theorem]
    Let $\{ F_n \}$ be a sequence of distribution functions converging to a distribution function $F$ at all continuity points $x$ of $F$.  This happens if and only if $\phi_n(t) \rightarrow \phi$ pointwise and $\phi$ is continuous in the neighbourhood of the origin.  If this is the case then $\phi$ is the characteristic function associated with $F$, and is continuous everywhere.
\end{thm}  

\begin{proof}
    If $\{ F_n \}$ converges to $F$, use Lemma \ref{lem:extendedhelly}, with $g(x) = \cos(xt) + \sin(xt)$.
\end{proof}
% section lecture_17_thursday_5_may (end)

\section{Lecture 18 - Thursday 12 May} % (fold)
\label{sec:lecture_18_thursday_12_may}

\begin{thm}
    Assume $F_n \rightarrow F$ at continuity points of $F$, and associated characteristic function $\phi_n \rightarrow \phi$ pointwise.  If $\phi_n \rightarrow \phi$ and $\phi$ is continuous in a neighbourhood of 0, then $F_n \rightarrow F$ and $F$ is distribution function associated with $\phi$.
\end{thm}

\begin{proof}
    From previous lemma, there exists a non-decreasing, right continuous non-negative function $F$ such that $F_n \rightarrow F$.  We need to show $F$ is a distribution function, that is $F(+ \infty) - F(-\infty) \geq 1$.  By Parseval's relation, we have \[
        \frac{\sigma}{\sqrt{2\pi}} \int_\R e^{-izt} \phi(z) e^{-\frac{1}{2}\sigma^2 z^2}\, dz = \int_\R e^{-\frac{1}{2}\left(\frac{x-t}{\sigma}^2\right)} \, dF(x) \leq F(+\infty) - F(-\infty)
    \]  The left hand side is equal to \begin{align*}
        \E(e^{-i N_\sigma t} \phi(N_\sigma))
    \end{align*} where $N_\sigma \sim N(0, \frac{1}{\sigma^2})$.  Since \[
        \left|e^{-izt} \phi(t) \right| \leq 1
    \] Assume $\phi$ is continuous on $|t| < A$.  Then \begin{align*}
        \E(e^{-iN_\sigma t} \phi(N_\sigma)) &= \E(e^{-iN_\sigma t} \phi(N_\sigma) \given |N_\sigma| \geq A ) \cdot P(|N_\sigma| \geq A) \\
                        &\qquad + \E(e^{-iN_\sigma t} \phi(N_\sigma) \given |N_\sigma| < A) P(|N_\sigma| < A).
    \end{align*}  The first term tends to zero as $\sigma \rightarrow \infty$, as $P(|N_\sigma| \geq A) \rightarrow 0$ on $|N_\sigma| < A$.  Then the distribution function tends to \[
        G(x) = \begin{cases}
            0   &x < 0 \\
            1   &x \geq 0 
        \end{cases}
    \] as $\sigma \rightarrow \infty$.  
    
    \[
        \lim_{\sigma \rightarrow \infty}\frac{\sigma}{\sqrt{2\pi}} \int_\R e^{-izt} \phi(z) e^{-\frac{1}{2}\sigma^2 z^2}\, dz = \int_\R e^{-izt} \phi(z) \, dG(z) = 1
    \] by the extended Helly-Bragg theorem.
 \end{proof} 

\begin{cor}
    If $X_n$ has distribution function $F_n$ and characteristic function $\phi_n$, and $X$ has distribution function $F$ and characteristic function $\phi$.  Then the following are equivalent.
    \begin{enumerate}[i)]
        \item $F_n(x) \rightarrow F(x)$ at all continuity points $x$ of $F$.  
        \item $\phi_n(t) \rightarrow \phi(t)$ for all $t$, 
        \item $\E(g(X_n)) \rightarrow \E(g(X))$ for all real, bounded, continuous functions $g$. 
    \end{enumerate}
    In these cases we write $X_n \cid X$ ($X_n$ converges in distribution to $X$)
\end{cor}

\begin{cor}
    Suppose $X_n \cid X$.  If $h$ is any continuous real valued function, then $h(X_n) \cid h(X)$.
\end{cor}
\begin{proof}
    $X_n \cid X$ if and only if $\E(g(X_n)) \rightarrow \E(g(X))$.  Then $g(h(x))$ is real, bounded, and continuous.  Then \[
        \E(g(h(X_n))) \rightarrow \E(g(h(X))) \Rightarrow h(X_n) \cid h(x)
    \] for all $g$ real, bounded, continuous.
\end{proof}

\begin{thm}[Slutsky's theorem]
    IF $X_n \cid X$ and $Y_n \cip a$, then \[
        X_n + Y_n \cid X + a
    \]
\end{thm}
\begin{proof}
    Given $\epsilon > 0$, choose $x$ such that $x, x - a \pm \epsilon$ are continuity points of $F(x) = P(X \leq x)$.  Then \begin{align*}
        P(X_n + Y_n \leq x) &= P(X_n + Y_n \leq x, |Y_n - a | > \epsilon) + P(X_n + Y_n \leq x, |Y_n - a| \leq \epsilon)        \\
        &\leq P(|Y_n - a | > \epsilon) + P(X_n \leq x - a + \epsilon) \\
        P(X_n \leq x - a - \epsilon) &= P(X_n \leq x - a - \epsilon, |Y_n - a| > \epsilon) + P(X_n \leq x - a - \epsilon, |Y_n - a | \leq \epsilon) \\
        &\leq P(|Y_n - a| > \epsilon) + P(X_n + Y_n \leq x)
    \end{align*}  Taking limits as $n \rightarrow \infty$, we have \[
        P(X \leq x - a - \epsilon) \leq \lim_{n \rightarrow \infty} P(X_n + Y_n \leq x) \leq P(X \leq x - a + \epsilon)  
    \]  Since $x - a \pm \epsilon$ are continuity points of $F$, we have \[
        \lim_{n \rightarrow \infty} P(X-n + Y_n \leq x) = P(X \leq x - a).\qedhere
    \]
\end{proof}
\subsection{Central limit theorem} % (fold)
\label{sub:central_limit_theorem}

% subsection central_limit_theorem (end)
\begin{note}[Notation]
    Let $X_1, X_2, \dots$ are independent random variables with characteristic functions $\phi_1, \phi_2, \dots$ and distribution functions $F_1, F_2, \dots$.  Let $\E(X_i) = 0, \var(X_i) = \sigma_i^2 < \infty$, $i = 1, 2, \dots$.  Let \[
        S_n = \sum_{i=1}^n X_i, \qquad s_n^2 = \var(S_n) = \sum_{i=1}^n \sigma_i^2
    \] 
\end{note}

\begin{thm}[Lindeberg conditions]
    Let $\epsilon > 0$.  Then \begin{align*}
        L_n(\epsilon) &= \frac{1}{s_n^2} \sum_{i=1}^n \E \left(X_i^2 \indic(|X_i| > \epsilon s_n) \right)  \\
        &= \frac{1}{s_n^2} \sumni \int_{|x| > \epsilon s_n} x^2 \, dF_i(x) 
    \end{align*}  Then the \textbf{Lindeberg condition} is \[
        \forall \epsilon > 0, \quad L_n(\epsilon) \rightarrow 0 \quad \text{as $n \rightarrow \infty$}
    \]
\end{thm}
\begin{exmp}
    Assume $\E(|X_i|^3) < \infty$.  Then \begin{align*}
        L_n(\epsilon) &\leq \frac{1}{s_n^2} \sumni \E(X_i^2 \frac{|X_i|}{\epsilon s_n}) \\
            &= \frac{1}{\epsilon} \frac{1}{s_n^3} \sumni \E(|X_i|^3)
    \end{align*}
    
\end{exmp}

\begin{thm}[Liapounov's condition]
    \[
        \frac{1}{s_n^3} \sumni \E(|X_i|^3) \rightarrow 0 \quad \text{as $n \rightarrow \infty$}
    \]  From above, Liapounov's condition implies Lindeberg's condition.
\end{thm}

\begin{thm}[Central limit theorem]
    If for all $\epsilon > 0$, $L_n(\epsilon) \rightarrow 0$ as $n \rightarrow \infty$, then \[
        \frac{S_n}{s_n} \cid N(0, 1)
    \]
\end{thm}
\begin{proof}
    Preliminaries.
    \begin{enumerate}[(i)]
        \item If $|a_k| <\leq 1$ and $|b_k| \leq 1$ for all $k$, then \[
            \left| \prod_{i=1}^n a_i - \prod_{i=1}^n b_i \right| \leq \sum_{i=1}^n |a_i - b_i| 
        \] as $a_1 a_2 - b_1 b_2 = (a_1 - b_1) a_2 - b_1(a_1 - b_2)$ and use induction.  
        \item $\left|e^z - 1 - z \right| \leq \delta |z|, \delta > 0$, for $|z|$ sufficiently small.   
    \end{enumerate}
    
    It is sufficient to prove \[
        \phi_{S_n/s_n}(t) = \prod_{k=1}^n \phi_k(t/s_n) \rightarrow e^{-\frac{1}{2}t^2} \tag{$\ddag$})
    \] for all $t$.
    
    Now \begin{align*}
        |\phi_k(t/s_n) - 1| &= \left| \int (e^{\frac{itx}{s_n}} - 1 - \frac{itx}{s_n})dF_k(x) \right| \quad \text{as $\E(X_k) = 0$} \\
        &\leq \int \frac{t^2}{x^2}{2 s_n^2} \, dF_k(x) \\
        &= \frac{1}{2}\frac{\sigma_k^2}{s_n^2}t^2 \tag{$\star$}
    \end{align*}  Now \begin{align*}
        \sigma_k^2 &= \E(X_k^2 \indic{|X_k| \leq u s_n}) + \E(X_k^2 \indic{|X_k| >  u s_n}) \\
        &\leq (u s_n)^2 + s_n^2 L_n(u) 
    \end{align*} Hence \[
        \frac{\sigma_k^2}{s_n^2} \leq u^2 + L_n(u)
    \] and since there are no $k$ on the RHS, we have \[
        \max_{k \leq n} \frac{\sigma_k^2}{s_n^2} \leq u^2 + L_n(u)
    \]  By Lindenberg's condition, we have $L_n(u) \rightarrow 0$ as $n \rightarrow \infty$, and as $u$ was arbitrary, we have \[
        \max_{k \leq n} \frac{\sigma^2_k}{s_n^2} \rightarrow 0
    \] From Assignment 5, we know \[
        \exp(\phi_k(t) - 1)
    \] is a characteristic function.  Let $\delta \rightarrow 0$.  Then \begin{align*}
        \left| \exp(\sum_{k=1}^n (\phi_k(t/s_n)) - 1) - \prod_{k=1}^n \phi_k(t/s_n) \right| &\leq \sum_{k=1}^n \left| e^{\phi_k(t/s_n) - 1} - \phi_k(t/s_n) \right| \quad \text{by (i)} \\
        &\leq \delta \sum_{k=1}^n \left|\phi_k(t/s_n) - 1 \right| \quad \text{by (ii)} \\
        &\leq \frac{\delta t^2}{2}{\sum_{k=1}^n \frac{\sigma_k^2}{s_n^2}} \quad \text{by ($\star$)} \\
        &= \frac{\delta t^2}{2}. \quad \text{if $n$ is sufficiently large}
    \end{align*}
    
    By $(\ddag)$, we must show \begin{align*}
        \sum_{k=1}^n (\phi_k(t/s_n) - 1) + \frac{1}{2} t^2 \rightarrow 0
    \end{align*} that is, \[
        \sum_{k=1}^n \int \left( e^{itx/s_n} - 1 - \frac{itx}{s_n} + \frac{1}{2} \frac{t^2 x^2}{s_n^2} \right)\, dF_k(x) \rightarrow 0  \tag{$\dagger$}
    \]  The modulus of the integral in ($\dagger$) is bounded by \[
        \frac{1}{6} \left|\frac{tx}{s_n} \right|^3 \leq u \frac{|t|^3 x^2}{6s_n^2}
    \] if $|x| \leq u s_n$ and \[
        \frac{x^2 t^2}{2s_n^2} + \frac{x^2 t^2}{2s_n^2}
    \] when $|x| > u s_n$.  Hence the integral of $(\dagger)$ is bounded above by \[
        \frac{u|t|^3}{6} + \frac{t^2}{s_n^2} \sum_{k=1}^n \int_{|x| > u s_n} x^2 \, dF_k(x) = \frac{u |t|^3}{6} + L_n(u) t^2
    \] as the integral is the Lindeberg's condition.  
    
    Given $t, \epsilon > 0$, choose $u$ such that $\frac{u |t|^3}{6} < \frac{\epsilon}{2}$, and $N_0$ large enough such that $L_n(u) t^2 < \frac{\epsilon}{2}$ for $n > N_0$.  So the left hand side of $(\dagger)$ is bounded above by $\epsilon$, and so the result follows.  
\end{proof}

\begin{thm}[Partial converse of the central limit theorem]
    Suppose that $s_n \rightarrow \infty$ and $\frac{\sigma_n}{s_n} \rightarrow 0$ as $n \rightarrow \infty$.  Then the Lindeberg condition is necessary for \[
        \frac{S_n}{s_n} \cid N(0, 1).
    \]
\end{thm}

\begin{proof}
    By assumption, given $\epsilon > 0$ there exists $N_1 > 0$ such that \[
        \frac{\sigma_k}{\sigma_n} < \frac{\sigma_k}{\sigma_k} < \epsilon
    \] for $N_1 \leq k \leq n$ as $s_n^2 \leq s_k^2 (k \leq n)$.  We also have \[
        \frac{\sigma_k}{s_n} < \epsilon, k = 1, 2, \dots, N_1
    \] for $n > N_1$ as $s_n^2 \rightarrow \infty$.  Hence \[
        \max_{1 \leq k \leq n} \frac{\sigma_k}{s_k} \rightarrow 0
    \] as $n \rightarrow \infty$.  Assume $\frac{S_n}{s_n} \cid N(0, 1)$.  If $(5)$ holds then this convergence is equivalent to $(1) \iff (3) ( \Rightarrow (4))$ as $(1) \iff (3)$ requiring $(5)$, to ensure \[
        \left|\phi_{k}(\frac{t}{s_n}) - 1 \right|
    \]  can be made uniformly small.  
    
    The real part of $(4)$, \begin{align*}
        \sum_{k=1}^n \int \left( \cos(\frac{xt}{s_n}) - 1 + \frac{x^2 t^2}{2s_n^2} \right) \, dF_k(x)
        &\geq \sum_{k=1}^n \int_{|x > u s_n} \left( \cos(\frac{xt}{s_n}) - 1 + \frac{x^2 t^2}{2s_n^2} \right) \, dF_k(x)
    \end{align*}  For any $u > 0$, choose $t$ such that $\frac{x^2 t^2}{2 s_n^2} - 2 > 0$ if $|x| > u s_n$ (i.e. $t^2 > \frac{4}{n^2}$).  Continuing, we have \begin{align*}
        &\geq \sum_{k=1}^n \int_{|x| > u s_n} \left( \frac{x^2 t^2}{2 s_n^2} - 2 \right) \, dF_k(x) \\
        &\geq \sum_{k=1}^n \int_{|x| > u s_n} \left(\frac{x^2 t^2}{2s_n^2} - 2\frac{x^2}{u^2 s_n^2} \right) \, dF_k(x) \\
        &= \left(\frac{t^2}{2} - \frac{2}{u^2}\right) \frac{1}{s_n^2} \sum_{k=1}^n \int_{|x| > u s_n} x^2 \, dF_k(x) \\
        &= \left(\frac{t^2}{2} - \frac{2}{u^2}\right) L_n(u)
    \end{align*} Thus $L_n(u) \rightarrow 0$ as $n \rightarrow \infty$, that is, Lindeberg's condition holds.
\end{proof}

\begin{cor}
    Let $X_1, X_2, \dots$ IID with $\E(X_1) = 0, \var(X_1) = \sigma^2$.  Then \[
        \frac{S_n}{\sqrt{n \sigma^2}} \cid N(0, 1)
    \] Let $\bar X_k = \frac{S_n}{n}$.
\end{cor}

\begin{proof}
     We have $s_n^2 = n \sigma^2$.  For $\epsilon > 0$, we have \begin{align*}
        L_n(\epsilon) &= \frac{1}{n \sigma^2} \sum_{k=1}^n \E( X_k^2 \indic{|X_k| > \epsilon \sigma \sqrt{n}}) \\
        &= \frac{1}{\sigma^2} \E(X_1^2 \indic{|X_1| > \epsilon \sigma \sqrt{n}}) \rightarrow 0
    \end{align*} as $n \rightarrow \infty$ as $\E(X_1^2) < \infty$.
\end{proof}
% section lecture_18_thursday_12_may (end)

\section{Lecture 19 - Thursday 19 May} % (fold)
\label{sec:lecture_19_thursday_19_may}
The central limit theorem is about distribution functions.  It is not an automatic consequence that the derivatives (densities) converge.  

If $\frac{S_n}{s_n}$ has density $f_n(x)$ we need further conditions to ensure $f_n(x) \rightarrow \frac{1}{\sqrt{2 \pi}} e^{- \frac{1}{2} x^2}$ as $n \rightarrow \infty$.  

\begin{thm}
    If $X_i$ are $IID$ with characteristic functions $\phi(t)$ and $|\phi(t)|$ is integrable then \[
        f_n(x) \rightarrow \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2} x^2}
    \]
\end{thm}

\begin{exmp}[Densities not converging]
    Let $X_i$ have density \[
        f(x) = \frac{C}{x (\log x)^2}, \quad 0 < x < \frac{1}{2}.
    \] Then $\E(X^2) < \infty$ but $\sum_{i=1}^n X_i$ has an unbounded density on $(0, \frac{1}{2})$.
\end{exmp}

\subsection{Stable Laws} % (fold)
\label{sub:stable_laws}
\begin{defn}[Stable distribution]
    A distribution $F$ is said to be stable if it is not concentrated at one point, and when $X_1$ and $X_2$ are independent with distribution function $F$ and $a_1, a_2$ are arbitrary constants there exists some $\alpha > 0, \beta$ such that \[
        \frac{\alpha_1 X_1 + \alpha X_2 - \beta}{\alpha}
    \] has distribution function $F$.   
\end{defn}

\begin{exmp}
    If $X_1$ has a characteristic function $\phi(t)$ then \begin{align*}
        \alpha X_3 + \beta &= a_1 X_1 + a_2 X_2 \\
        e^{i \beta t} \phi(\alpha t) &= \phi(a_1 t) \phi(a_2 t)
    \end{align*}  If $\phi(t) = e^{-c|t|^\gamma}$, $0 < \gamma \leq 2$, then \[
        \phi(a_1 t) \phi(a_2 t) = e^{-c\left( |a_1|^\gamma + |a_2|^\gamma \right) |t|^\gamma}.
        \]

    As these distributions are symmetric, we have $\beta = 0$, and so setting $\alpha = (|a_1|^\gamma + |a_2|^\gamma)$.  Thus distributions with characteristic functions of the form $e^{-c|t|^\gamma}$ are stable.  Hence the Cauchy distribution is stable ($\gamma = 1$), and the normal distribution is stable $(\gamma = 2)$. 
\end{exmp}

\begin{thm}
    If $\phi$ is the characteristic function of a symmetric random variable $(X \eqd -X)$ with a stable distribution then $\phi(t) = e^{-c|t|^\gamma}$ for some $c > 0$, $\gamma \in (0, 2]$.  
    
    Recall that a distribution is symmetric if and only if $\phi$ is real.
\end{thm}
\begin{proof}[Partial]
    $\phi(t) \phi(t) = \phi(\alpha t)$ used to show that $\phi(t) \neq 0$.  (Since $\phi(0) = 1$ and $\phi(t)$ is continuous).
    
    Then build up properties of $\phi$.
\end{proof}

\begin{thm}[L\`evy]
    Let $X_1, X_2, \dots$ be independent and identically distributed random variables with distribution functions $G$.  Let $S_n = \sum_{i = 1}^n X_i$.  
    Suppose that there exists a sequence of constants $(a_n, b_n)$ with $b_n > 0$, such that \[
        \frac{S_n - a_n}{b_n} \cid X
    \] where $X$ is not a constant.   Then $X$ is stable.
\end{thm}

\begin{defn}[Domain of attraction]
    If $X$ has distribution function $F$ then we say $G$ is in the \textbf{domain of attraction} of $F$.    
\end{defn}
\begin{cor}
    If $X$ has finite variance then $G$ is in the domain of attraction of the normal distribution.  
\end{cor}

\begin{cor}
    If $G$ satisfies $\lim_{x \rightarrow \infty} x(1 - G(x)) = c > 0$ then $G$ is in the domain of attraction of the Cauchy distribution, that is, \[
        x\,\P(X > x) \rightarrow c.
    \]  A necessary and sufficient condition to be in the domain of attraction for the Cauchy distribution is \[
        1 - G(x) = P(X_1 > x) = \frac{L(x)}{x}
    \] where $L(x)$ is a \textbf{slowly varying function}.  $L(x)$ is a slowly varying function if for all $C > 0$, \[
        \lim_{x \rightarrow \infty} \frac{L(C x)}{L(x)} = 1.
    \]  For example, $L(x) = 1, L(x) = \log x$ are slowly varying functions.
\end{cor}

\begin{thm}
    All stable laws are absolutely continuous and the distribution functions have derivatives of all orders.
\end{thm}

\begin{thm}
    The normal distribution is the only stable law with finite variance.
\end{thm}

\begin{thm}
    It can be shown that the canonical form of the characteristic function of a stable law is \[
        \phi(t) = \exp\left[ i \gamma t - c|t|^\gamma \left\{ 1 + \frac{i \beta t}{|t|} \omega(t, \alpha)\right\}\right]
    \]  where \begin{align*}
        \gamma \in \R, \alpha \in (0, 2], c > 0, |\beta | \leq 1, \omega(t, \alpha) = \begin{cases}
            \tan \frac{\pi \alpha}{2} & \alpha \neq 1 \\
            \frac{2}{\pi} \log |t|  &\alpha = 1
        \end{cases}
    \end{align*}
    
    If $\phi$ is real, then $\beta = \gamma = 0$. 
\end{thm}
% subsection stable_laws (end)  
% section lecture_19_thursday_19_may (end)

\section{Lecture 20 - Thursday 19 May} % (fold)
\label{sec:lecture_20_thursday_19_may}
\subsection{Infinitely divisible distributions} % (fold)
\label{sub:infinitely_divisible_distributions}

% subsection infinitely_divisible_distributions (end)
Consider a triangular array $\{ X_{nk} \}_{k=1}^n$ where for each $n$, $X_{n1}, X_{n2}, \dots, X_{nn}$ are independent random variables.  We assume that the distribution are identically distributed for each $n$.\[  \begin{matrix}
    X_{11} &        & \\
    X_{21} & X_{22} &   \\
    X_{31} & X_{32} & X_{33} \\
    \vdots &        &       &\ddots
\end{matrix}
\]

\begin{exmp}
    Let $X_{nk} \sim B(1, p_n)$.  Then $S_n = \sum_{k=1}^n X_{nk} \sim B(n , p_n)$.  We know that if $n p_n \rightarrow \lambda$ as $n \rightarrow \infty$, then \[
        S_n \cid \textsc{Poisson}(\lambda).
    \]
    
    Note that the Poisson distribution is not continuous, nor is it stable.  Consider $X_1, X_2$ Poisson distributed, and let $Y = 2X_1 + 3X_2$.  Then $Y$ is not in the Poisson family as $P(Y = 1) = 0$.  
\end{exmp}

\begin{defn}[Infinitely divisible]
    A distribution function $F$ is infinitely divisible if for every positive integer $k$, $F$ is the $k$-fold convolution of some distribution $G_k$ with itself.  
\end{defn}

\begin{exmp}
    \begin{enumerate}[(1)]
        \item The Poisson distribution is infinitely divisible, as \[
            \phi(t) = e^{\lambda(e^{it} - 1)} = \left[ e^{\frac{\lambda}{k}(e^{it} - 1)} \right]^k
        \]
        \item Symmetric stable laws are infinitely divisible, as \[
            \phi(t) = e^{-c|t|^\alpha} = \left( e^{-\frac{c}{k}|t|^\alpha} \right)^k
        \]
    \end{enumerate}
\end{exmp}

\begin{lem}
    Assume $X_n \cid X$, $Y_n \cid Y$, $\{ X_n \}, \{ Y_n \}$ independent.  Then \[
        X_n + Y_n \cid X + Y.
        \]  
\end{lem}
\begin{proof}
    $X_n$ has a characteristic function $\phi_n(t) \rightarrow \phi(t)$.  $Y_n$ has characteristic function $\psi(t) \rightarrow \psi(t)$. Then \[
        \phi_n(t) \psi_n(t) \rightarrow \phi(t) \psi(t).
    \]   
\end{proof}

\begin{thm}
    Given the array $\{ X_{nk} \}$, letting $S_n = \sum_{k=1}^n X_{nk}$.  If $P(S_n \leq x) \rightarrow F(x)$ then $F$ is infinitely divisible.  
\end{thm}

\begin{proof}   
    Fix $k$.  We must show that $F$ is the $k$-fold convolution of some $G_k$.  Let $n' = mk$, $m = 1, 2, \dots$, and let \[
    Y_i^{(m)} = X_{n', (i - 1)m + 1} + \dots + X_{n', im}, \quad i = 1, \dots, k.   
    \]  Then \[
        S_{mn} = Y_1^{(m)}  + \dots + Y_k^{(m)} 
    \] and $Y_f^{(m)}$ are IID.


    If $P(Y_1^{(m)} \leq x) \rightarrow G_k(x)$ as $m \rightarrow \infty$ then \[
        G^{*k}_k = F
    \]  So we need to show that $G_k$ is a well defined distribution.  We have $Y_{1}^{(m)}$ is the sum of $m$ iid random variables, and \[
        H_m(x) = P(Y_1^{(m)} \leq x).
    \]  We need to ensure ``no probability escapes to infinity.''  Given a convergent subsequence of distribution functions, we know that the limit satisfies $G_k(x), G_k$ right continuous, non-decreasing.  We need to show $G(+\infty) = 1$.  Suppose that there exits $\epsilon > 0$ such that for any $M> 0$ we can find a subsequence $m'_n$ such that \begin{align*}
        P( |Y_1^{(m'_n)}| > M ) > \epsilon
    \end{align*}
    
    There is a subsequence of $\{ m'_n \}$, $\{ m''_n \}$ say, such that \[
        P(Y_1^{m_n''} > M) > \frac{\epsilon}{2} \quad \text{or} \quad P(Y_1^{m_n''} < -M) > \frac{\epsilon}{2}
    \] So \[
        P(Y_1^{m_n''} + \dots + Y_k^{m_n''} > kM) > \left(\frac{\epsilon}{2}\right)^k 
    \] and so $F(km) \leq 1 - \left( \frac{\epsilon}{2}\right)^k$ (modulo choosing continuity points $kM$ of $F$).  Now, since we know that our limiting distribution $F$ is a proper distribution function, we obtain our contradiction (no such $\epsilon > 0$ exists).  
    
    Hence $G_k$ is a proper distribution function, and so $G_k^{*k} = F$.    
\end{proof}
\begin{defn}[Compound Poisson distribution]
    Let $X_1, X_2, \dots$ IID random variables.  and let $N \sim \textsc{Poisson}(\lambda)$.  Then let $S_N = X_1 + \dots + X_N$.  Then $S_N$ has a compound Poisson distribution.  If $X$ has characteristic function $\phi$, then $S_N$ has characteristic function \begin{align*}
        \E(e^{itS_N}) &= \sum_{n = 0}^\infty \E(e^{itS_N} \given N = n) P(N = n) \\
                    &= \sum_{n = 0}^\infty \phi(t)^n e^{- \lambda} \frac{\lambda^n}{n!} \\
                    &= e^{- \lambda} e^{\lambda \phi(t)} \\
                    &= e^{\lambda(\phi(t) - 1)}.
    \end{align*}  The compound Poisson distribution is clearly infinitely divisible.  
\end{defn}

\begin{thm}
    A distribution function $F$ is infinitely divisible if and only if it is the weak limit of a sequence of distributions, each of which is compound Poisson.
\end{thm}
% section lecture_20_thursday_19_may (end)

\section{Lecture 21 - Thursday 26 May} % (fold)
\label{sec:lecture_21_thursday_26_may}
\begin{thm}
    A distribution function is infinitely divisible if and only if it is the weak limit (limit in distribution) of a sequence of distribution functions each of which is compound Poisson.
\end{thm}

\begin{lem}
    \label{lem:infd1}
    The weak limit of a sequence of infinitely divisible distributions is infinitely divisible.
\end{lem}

\begin{proof}
    Let $F_n(x)$ be a sequence of distribution functions that are infinitely divisible with \[
        F_n(x) \rightarrow F(x)
    \] at all continuity points $x$ of $F$.  Form an array $\{ X_{nk} \}_{k=1}^n$ where for a given $n$, $X_{n1}, X_{n2}, \dots, X_{nn}$ are IID with distribution function $_nF_n(x)$, the \emph{n\textsuperscript{th} root of $F_n$}.  Then \[
        S_n = \sum_{k=1}^n X_{nk}
    \]   has distribution function $F_n$.

    We know $F_n(x) \rightarrow F(x)$ so from the previous result $F$ is infinitely divisible as it is the limit of the row sums of a triangular arrow of row-wise infinitely divisible random variables. 
\end{proof}

\begin{lem}
    \label{lem:infd2}
    The characteristic function of an infinitely divisible distribution is never zero.
\end{lem}

\begin{proof}
    If $\phi(0) = 1$ and $\phi$ is continuous, without loss of generality assume $\phi$ is real (if not, consider $|\phi|^2 = \phi \overline\phi$ which is real and infinitely divisible.)  
    
    Let $\phi_k(t)^k = \phi(t)$.  Assume $\phi(t) > 0$ for $|t| \leq a$.  Then for $t \in (-a, a)$, $\phi_k(t) \rightarrow 1$ as $k \rightarrow \infty$. 
    
    Now note that \[
        1 - \phi(2t) \leq 4(1 - \phi(t)), \tag{$\star$}
    \] as \begin{align*}
        1 - \phi(2t)    &= \int (1 - \cos 2tx  \, dF(x) \quad \text{as $\phi$ is real }\\
                        &= \int (2 -  2\cos^2 tx) \, dF(x) \quad \cos 2 \theta = 2 \cos^2 \theta - 1 \\
                        &= 2  \int(1- \cos tx) (1 + \cos tx) \, dF(x) \\
                        &\leq 4\int(1 - \cos tx) \, dF(x) \quad 1 - \cos tx \geq 0 \\
                        &= 4(1 - \phi(t))
    \end{align*} as required.

    Then we have $1 - |\phi(2t)| \leq 1 - \phi(2t)|^2 \leq 4(1 - |\phi(t)|^2) \leq 8(1 - |\phi(t)|)$.  If $\phi(t) \neq 0$ on $0 < t < a$ and $\epsilon > 0$ arbitrary, we can find $k$ large enough such that \[
        1 - |\phi_k(t)| < \frac{\epsilon}{8}
    \]  which implies $1 - |\phi_k(2t)| < \epsilon$ and so $|\phi_k(2t)| \neq 0 $ on $|t| < a$.  So $\phi_k(t)| \neq 0$ on $|t| < 2a$, and hence $|\phi(t)| \neq 0$ on $|t| < 2a$.  
    
    Iterating this argument, we have that $|\phi(t)| > 0$ for all $t$.  
\end{proof}

\begin{lem}
    \label{lem:infd3}
    For each $k$, let $\phi_k$ be a characteristic function such that $\phi_k^k(t) = \phi(t)$.  $\phi(t)$ is a characteristic function of an infinitely divisible distribution.  Then $lim_{k \rightarrow \infty} \phi_k(t) = 1$ for all $t$.  
\end{lem}
\begin{proof}
    Since $\phi$ is continuous and $\phi(0) = 1$, we have \[
        | \phi_k(t) | = |\phi(t)|^{1/k} \rightarrow 1 
    \] as $k \rightarrow \infty$.  
    
    We have $k \arg \phi_k(t) = \arg \phi(t) + 2 \pi j, j = 0, 1, \dots, k-1$.  Since \begin{align*}
        \arg \phi_k(0) = \arg(1) = 0 \quad \text{so $j = 0$} \\
        \arg \phi_k(t) = \frac{1}{k} \arg \phi(t) \rightarrow 0 
    \end{align*} as $k \rightarrow \infty$, and so $\phi_k(t) \rightarrow 1$ as $k \rightarrow \infty$.  
\end{proof}

\begin{proof}[Proof of theorem]
    Let $\phi$ be the characteristic function of an infinitely divisible distribution $F$.  Let $\phi_k^k(t) = \phi(t)$.  Then \begin{align*}
        \log \phi(t)    &= k \log \phi_k(t) \\
                        &= k \log ( 1 - (1 - \phi_k(t))) 
    \end{align*}
  Since $ 1 - \phi_k(t) \rightarrow 0$ as $ k \rightarrow \infty$, we have \begin{align*}
        \log \phi(t) &= -k [ 1 - \phi_k(t) + \frac{\left( 1 - \phi_k(t) \right)^2}{2} + \dots] \\
            &= - k[1 - \phi_k(t)]( 1 + \frac{1 - \phi_k(t)}{2} + \dots) \\
            &=  - k[1 - \phi_k(t)] + o(1)
    \end{align*} and so $\phi(t) \sim e^{-k( 1 - \phi_k(t))}$ which is a compound Poisson characteristic function. 
\end{proof}

\begin{exmp}
    Show that the $U([-1, 1])$ distribution is not infinitely divisible.  This has associated characteristic function $\frac{\sin t}{t}$.  Then $\phi(\frac{\pi}{2}) = 0$, and so the distribution is not infinitely divisible.
\end{exmp}
% section lecture_21_thursday_26_may (end)

\section{Exam material} % (fold)
\label{sec:exam_material}
\begin{itemize}
    \item Borel-Cantelli lemma.
    \item Martingales, central limit theorems, strong law of large numbers.
    \item Inequalities of random variables.
\end{itemize}

\begin{exmp}[Q2b) of 2010 Exam]
    Let $(X_j)$ be IID.  Then \[
        \E|X_1| < \infty \iff \P(|X_n| \geq n \text{ i.o }) = 0 
    \]  We have \begin{align*}
        \E|X_1| < \infty &\iff \sum_{j=1}^\infty \P(|X_1| \geq j) < \infty \\
                &\iff \sum_{j=1}^\infty \P(|X_j| \geq j) \quad \text{by IID} \\
                &\iff \P(|X_j| \geq j \text{ i.o }) = 0
    \end{align*} by Borel-Cantelli lemma.
\end{exmp}

\begin{exmp}[Q7 of 2010 Exam]
    Let $\{ X_n \}$ be a sequence of IID random variables on a probability space $(\Omega, \sigf, P)$ with \[
        P(X_1 = 1) = P(X_1 = -1) = \frac{1}{2}.
    \]  Let $\sigf_n = \sigma(X_1, X_2, \dots, X_n)$ and let $\{B_n\}$ be a sequence of events with $B_n \in \sigf_n$, satisfying \[
        B_1 = \Omega, \lim_{n \rightarrow \infty} P(B_n) = 0, P(\limsup B_n) = 1.
    \]  Define $Y_1 = 0$ and \[
        Y_{n+1} = Y_n(1 + X_{n+1}) + \mathbf{1}_{B_n} X_{n+1}, n = 1, 2 \dots.
    \]
    \begin{enumerate}[(a)]
        \item Show that $\{ Y_n \}$ is a martingale.
        \item Show that $Y_n$ converges in probability to 0.
        \item Show that $\limsup B_n \subseteq \limsup \{ Y_n \neq 0 \}$ and hence show that $\{ Y_n \}$ does not converge almost surely.
    \end{enumerate}
\end{exmp} 
\begin{proof}{\ }\begin{enumerate}[(a)]
    \item Note that $Y_1$ is $\sigf_1$-measurable.  By induction, we have that $Y_n+1$ is $\sigf_{n+1}$-measurable.  
    
    We have \begin{align*}
        \E|Y_{n+1}| &\leq 2 \E|Y_n | + P(B_n)  \quad \text{as $|X_{n+1} \leq  1$}\\ 
    \end{align*}   as $\E|Y_1| = 0$, $P(B_n) \leq 1$, so by induction, $\E|Y_n| <\infty$ for all $n$.  
    
    Finally, \begin{align*}
        \E(Y_{n+1} \given \sigf_n) &= Y_n \E(1 + X_{n+1} \given \sigf_n) + \mathbf{1}_{B_n} \E(X_{n+1} \given \sigf_n) \\
                    &= Y_n \quad \text{as $\E(X_{n+1} \given \sigf_n) = \E(X_{n+1}) = 0$.}
    \end{align*}  Hence $Y_n$ is a martingale.
    \item Let $\epsilon > 0$.  We must show $P(|Y_n| > \epsilon) \rightarrow 0$ as $n \rightarrow \infty$.  Consider $P(Y_{n+1} \neq 0)$. We have \begin{align*}
        P(Y_{n+1} \neq 0) &\leq P(\text{$B_n$ occurs or $Y_{n} \neq 0$ and $X_{n+1} = 1$)} \\
                        &= P(B_n) + \frac{1}{2} P(Y_n \neq 0).
    \end{align*}  Hence \[
        \lim_{n \rightarrow \infty} P(Y_{n+1} \neq 0) \leq 2 \lim_{n \rightarrow \infty} P(B_n) = 0
    \] and so $Y_n \cip 0$.  
    \item If $Y_n \cas Y$ almost surely then by uniqueness of limits in probability $Y = 0$ almost surely.  We have \[
        Y_{n+1} = \begin{cases}
            2Y_{n} + \mathbf{1}_{B_n} & X_{n+1} = 1 \\
            -\mathbf{1}_{B_n} & X_{n+1} = -1
        \end{cases}
    \]  Hence $B_n \subseteq \{ \omega : Y_{n+1}( \omega) \neq 0 \}$.   Thus \begin{align*}
        \limsup B_n = \bigcap_{m}^\infty \bigcup_{n=m}^\infty B_{n} \subseteq \bigcap_{m}^\infty \bigcup_{n=m}^\infty \{ Y_{n+1} \neq 0 \} \\
        &= \limsup \{Y_n \neq 0 \}
    \end{align*}  Hence $1 = P(\limsup B_n) \leq P(\limsup \{ Y_n \neq 0 \})$ and so $P(Y_n \neq 0 \text{ i.o.}) = 1$, and so $Y_n$ does not converge almost surely.
\end{enumerate}
    
\end{proof}
% section exam_material (end)














\end{document}